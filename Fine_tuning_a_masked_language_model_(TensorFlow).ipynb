{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Stv4LqlNE74N"
   },
   "source": [
    "# Fine-tuning a masked language model (TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbUM7QrGE74l"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f68QlMGE74m",
    "outputId": "2b9abe1c-bcaa-4d24-d0d8-5b5d0e316cce"
   },
   "outputs": [],
   "source": [
    "!pip -q install datasets evaluate transformers[sentencepiece]\n",
    "!apt install git-lfs\n",
    "!pip -q install --upgrade fsspec\n",
    "!pip -q install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdSKDRkmE74o"
   },
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HetiNiqUE74p"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"your email\"\n",
    "!git config --global user.name \"your name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPNUUWd3E74q"
   },
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "e786c205644c430abe1447591c6c98d4",
      "fc47944d46ea4a64bb1211525dd4514c",
      "a1f422980167454eb14fcd8f98b02f34",
      "6a5a09adbd5a454d91f7d381c0d88026",
      "bc3e296acf0a4c6f8ef405d963b839ed",
      "346221e7aa00481093f7f2e8fd51bf12",
      "78bd6f3accee4682b3924291ab0127ce",
      "390863c331e54653bc582cc7178e77be",
      "7ce98eb61d0642739c9133ada6e2ed0f",
      "b4bfc498706e4bb4984f297b65da3f21",
      "e733937f0d2143c7a110f5f0c7c127ff",
      "293069ea527a48668b7dc461a1e3f946",
      "f18e1cd6956942388116d13f0260205f",
      "81e7b369d2a646be9fae23bf1db656ca",
      "1503ebe881bd44fc8de3a991124812c6",
      "a2866233b963459bb5826d9d91aa111f",
      "4c8e37c3706345aba108d6b191931eac",
      "e7f8203cf1ca44daa357c6e7a9f961e6",
      "28d3358d261e491a979a9d5fe6835495",
      "e2937718611d444abf221e94c301047d"
     ]
    },
    "id": "ZXOguMljE74q",
    "outputId": "a8b79da5-644e-401c-8205-81e7bbb3a02a"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXvvLV0AorZJ"
   },
   "source": [
    "# Download DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 150,
     "referenced_widgets": [
      "65e2e0d893af465fae222782efff3b11",
      "5b46963f885446cc8f8b97f1268dec7e",
      "bc8350d7e90d40768ca2003090326286",
      "45301f31cd134757802ef8378c6fde3d",
      "1f9e79130a554f9ababbc22a6ef69782",
      "ec2c6874f29345849a2f2201b5dc0a98",
      "6bfc34eb70014dc687600de48b57b416",
      "2da9a31d9ec742f68e13a4432c8f1de7",
      "b0100c8adcd2494395b4b20b9d628e19",
      "dd91c95044ee42caaa6e48b68108a8f5",
      "2f6de51075d941e89923eed1e4385786",
      "3662a6bf072a42709569a085eb20ae7a",
      "4076fd00660247fdbafc02fbe54320ce",
      "c78b03aee26146d2b37da0dd52772ece",
      "2739a0edfad74615b139109af39b0dab",
      "c77e03f0278746b8b29a900589a29bdd",
      "7341fb8349eb46d386cebe73c89f76ef",
      "7a507c614cef44f98ac386c6f93b89fe",
      "4e2ce6a57a43438c942dd653561bed59",
      "9bff4754cdd1415e97b716cb3c32f3f1",
      "064e4f3cf291447ebd4076b9d0f0cff0",
      "16510c0b5ca742d0a4fdb6dd8a17b482"
     ]
    },
    "id": "cMoYOPRjE74r",
    "outputId": "347cd522-3c8f-4338-818a-b69c6d47a7f4"
   },
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_mZB524E74s",
    "outputId": "615e4c71-f853-463c-be7e-a408e68e46d7"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LalFRZ1WE74w"
   },
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "7236353b8bad43d19cfd23657f2ccdee",
      "d75e806887df4fbc91fe871f1693a1de",
      "de7f5e1c39db4842ab28f03a470e8812",
      "1b2491c2c89e4a0b9178ea6d0e718179",
      "89916306bc25456099a286a1102093d7",
      "4b0bcd133d7c4908b751eb565bfc9afa",
      "ad8b7fd160cc46dca71b0d65ad61d8aa",
      "d17571aa53f447188dff31fda1c0e703",
      "f1c7b9d058624892b27e273c3b5ef253",
      "9113abdf90ed49d3ad66098bdec69106",
      "d9734a2ae7d043e09412ecf0148ebe71",
      "12f55820c19d4c08a3b7fb9e970322e0",
      "94298afd96f143e691eb96bd092d1561",
      "7b36dcb2cec84c3db556918ec3eba662",
      "a748812803b7497a8194a8fb3f4fd161",
      "02c3f27c221a4ecb939eeeda94408982",
      "03206c786510430683a15a05699f32d7",
      "1572b006d30b49d99b451496b48021e5",
      "4741f81a9706412ba1bee0c7253f633f",
      "5f1c931067d2456daa60e8df932079f1",
      "090445757e9146158bd850132ededf51",
      "92716461221843cd9d8ced29331d67fb",
      "7289a817921a4674a1aaabcd10018461",
      "d9b092b570c14460962970a722a332c2",
      "e0a004486b0849d489e6a5415e459789",
      "5c2260fd9e514ba9bd33375abc2d5ca5",
      "3e3f7f7851784389b295350a2eef3a76",
      "2820a1197e974191823a32d4f30790de",
      "3d86d9877c03483da82f96ce6faba45a",
      "7a97c759b5e94728b6af0103da4866ce",
      "3f0df7430fe44efcbcc5b979713c167e",
      "049c2e2d41cb427cb8afa904006617d4",
      "6c3290dffbcf4b28ad66550ced3ee538"
     ]
    },
    "id": "5HOAqjRJE740",
    "outputId": "fa1852de-523a-4078-ec3e-0ccf9c180e99"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1RGdor4qeyZ",
    "outputId": "f8d6fb71-624e-4cb7-b7c0-5cee33f7a32c"
   },
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E869wfksE740",
    "outputId": "16f0caac-2853-413b-cf01-6d3bf6cb1d6e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "token_logits = model(**inputs).logits\n",
    "# print(token_logits)\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = np.argwhere(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "# We negate the array before argsort to get the largest, not the smallest, logits\n",
    "top_5_tokens = np.argsort(-mask_token_logits)[:5].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srbTyP2XrQlI"
   },
   "source": [
    "# The dataset\n",
    "## Large Movie Review Dataset (or IMDb for short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484,
     "referenced_widgets": [
      "3b7571ef69dc4391864ec49fc5e8faa8",
      "fc2455b2d2294f81a72a26ebbf27b9e6",
      "95b862dbc64e4c489a06fe8207f75dc7",
      "b6e9f2457e4444879916aaa3f2af955a",
      "fbd9998891ec4262875a068e4b10d2d1",
      "e7eadf22728e41dc9a45160c71ef6afe",
      "dc228796d1fa43d590dad81d83789e12",
      "efc963bf00e0436b8474f5670ca32910",
      "9178508e931d4423936c71d89584ee3a",
      "69b4931927fb45588cbb642a5c55d92c",
      "62ae3062189248599cfce2eaf5a94495",
      "8504e11eccc64cb28e9377f56bce9501",
      "1ede9d5a6bb94483bdd660cff097f5d8",
      "2ead8ea09a9342148609c68beb7fdeb2",
      "2a0ff56dab184af0aad69991bb8cb186",
      "0787a785628b4e6d94fc56aff37ab57c",
      "26751d84cb804936b298720fb2fda4f9",
      "eaafbfd4740a4e3d85f48cf535fce36a",
      "a4b1a2b963d047c78037b44edfa7d752",
      "3c300c9e32974c03b8da1ff405f30a89",
      "13725f41345049708282a2c43fc0984c",
      "1f60deb72ac3481cb15d8772ebd931ad",
      "c14a2445840b4a1a8e87ed2b7caf05fd",
      "99eb7c11ac0e4442b9fd5d478e18a275",
      "bcda6ff1224c47868731516127200c77",
      "5867e41543164c60a92c3266a3f19657",
      "e9689d52dd2b4d25a8143520db6069b0",
      "cbb1c8b8bb224915a69c02e210a3d8af",
      "2097d94fe49d44f49c379bc62a8b9c99",
      "bcfdeacdda7f46d39fd48aaa1bdac267",
      "53544198d1d0415195b1a9035fac5f38",
      "9c71eae337b6431fb3253ab93a5ddad0",
      "9ca0a199cd994598a02f8f000d6b141f",
      "24841d28f4b34af6b5f5f11da331ae98",
      "ff0634c8c4dc417f904510fb55f40b17",
      "fb5ace67100e4f41b8ceca3726450888",
      "f2b07f0c35c74331a7739c72a0ac4811",
      "ba5657b84fc04572b4eed9a2f716b038",
      "98ec190d2d3e47ebbb3f5b0ae6cea289",
      "07693125eb974168bb92130eccd3113c",
      "85e8c9cbb815428185ad4daa1d8c0d6a",
      "de7d24c1fccc4334bbb453593016cc71",
      "255ee765aa264f1e9d6c02db0acc060e",
      "6602177f578040fdaf34ead02c6ffde7",
      "7f25dc7ae78f4f04942d77ae78a54cfb",
      "bffd18aa823b4484acc9697b713b50b4",
      "f0edbfb9039749faba12d938e52bd310",
      "113ef656406c43de89a29a16f68722d7",
      "fff627a5c4ec411cb13899fd6ce8a7a4",
      "ed2dfdab13654e0a839400552795ee21",
      "a85c708c58cd44efb408fb6090b34dd6",
      "6661adf1809648ad8b99da05c66b8af0",
      "d204a0a04ac24b8892c9ceeb4acf9a32",
      "952711d38bf645f9b817dcb6257b797f",
      "782e7076aa5d4ec3b9de711e65001a97",
      "f774f11bfb834d6fa70ff0d6f6e5feba",
      "75ef9fd329ad4ee8b77fa41e6f06b96d",
      "8557827555e04ed9a0c9f87938866039",
      "cf3e046173684b9ca56a507f60f7202b",
      "328c6e92cc1d47afb50372eb1326ed98",
      "ae120828e7e14eaea65f50b4902ccc73",
      "48269254f8a14a4fa700801fb3cdd1fa",
      "61ff88cc88344a6f98b78e5370c5be35",
      "bd0bee0af8fd44f58ac6a0693cd06ec3",
      "fbf18064f762477cace16cbb0c33f2ae",
      "82bd14e8c7314bac9c8c95e12e610fb7",
      "f779ca270c6841ab8f569abf487cd22f",
      "85f655a984b743d4957326f25728eb4b",
      "cee2a5345622422eac7c4b97f641d3b6",
      "6895587b703b4501a816c5c604ecccfb",
      "d9df8dc2fee54d1b8c0da7e858ee3f80",
      "47b14c5c0c854942841d2eee90de1903",
      "d3b360b6202f4b88aa38f389989bb9de",
      "7ac98a8a868947a68368842c27e902c5",
      "c902b12358ec4c60a3c7e217473f566e",
      "db7df89b3529491aafc3f5e8072532a1",
      "0d243e39b4dc48869b08e88ae1a2b1bb"
     ]
    },
    "id": "cBK7G8wlE741",
    "outputId": "2fe2bd5e-b08d-40cf-8ab9-a0cf64a861c9"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ev8nvMfcE743",
    "outputId": "474cb6da-eceb-4528-8f94-b308924c0b0c"
   },
   "outputs": [],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZJPAvhKr01R",
    "outputId": "d4aa652b-4864-4ce3-ebf2-09fec199e21c"
   },
   "outputs": [],
   "source": [
    "label = imdb_dataset[\"train\"].features[\"label\"]\n",
    "print(f\"label:{label}\")\n",
    "\n",
    "label_names = label.names\n",
    "print(f\"label_names:{label_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bk9XygzltKfY"
   },
   "source": [
    "**✏️ Try it out!** Create a random sample of the unsupervised split and verify that the labels are neither 0 nor 1. While you’re at it, you could also check that the labels in the train and test splits are indeed 0 or 1 — this is a useful sanity check that every NLP practitioner should perform at the start of a new project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URgp3D3ptNG_",
    "outputId": "fd08e674-08f0-4ce2-a5f0-0def86735ace"
   },
   "outputs": [],
   "source": [
    "sample = imdb_dataset[\"unsupervised\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample:\n",
    "    print(f\"\\n'>>> Review: {row['text']}'\")\n",
    "    print(f\"'>>> Label: {row['label']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sF7xdIYStslp"
   },
   "source": [
    "**Sanity Check for Label Values.**\n",
    "To verify that label is not 0 or 1 in the unsupervised split (and that train/test splits do have binary labels), you could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rK7GE8NEtpdf",
    "outputId": "30059e70-db70-4e41-9a14-7850a81db20a"
   },
   "outputs": [],
   "source": [
    "# Check label values in all splits\n",
    "print(\"Train label example:\", imdb_dataset[\"train\"][0][\"label\"])\n",
    "print(\"Test label example:\", imdb_dataset[\"test\"][0][\"label\"])\n",
    "print(\"Unsupervised label example:\", imdb_dataset[\"unsupervised\"][0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Oh3TvlftsXJ"
   },
   "source": [
    "Or even better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q2jzHHPLtvtw",
    "outputId": "ae82f771-8415-41b3-ef41-fee7563f6cd3"
   },
   "outputs": [],
   "source": [
    "print(\"Unique labels in train:\", set(imdb_dataset[\"train\"][\"label\"]))\n",
    "print(\"Unique labels in test:\", set(imdb_dataset[\"test\"][\"label\"]))\n",
    "print(\"Unique labels in unsupervised:\", set(imdb_dataset[\"unsupervised\"][\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbKBuWj9ukNp"
   },
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373,
     "referenced_widgets": [
      "72ca1a0eeddd4a4fb9a9477341afb623",
      "6af472f8d09944f88d071c74e5557e44",
      "5cea57f057d54d029747513617419b25",
      "3929797c2e284e248efd0b6243a24ab1",
      "fa4348e7a79e4ed9bbc010acdcb32473",
      "29e047e9960049059ca3ee3b6438953c",
      "c617da16799c40e7bafa56481e628550",
      "184fedfdd3a94185b4aa1b353e753f14",
      "66f375ea99f4404c87a001c468c2554b",
      "c33adf3bc8284be39bb5570d12c722b4",
      "38bd97a050db4d9e8659f84b4827dd64",
      "ce4ebf8d17be401a8ab396a6363a8fa4",
      "2e4aa7fa6e8b406a8af643e8053311f4",
      "30fe15707c524a50b2192aa2ac94ab2f",
      "7d65e07eb3994336a48917398cbd88e3",
      "c6c77049ce6646c4b6fd7a1afc36a7ac",
      "d009812e0c81466e83e540a815611b0a",
      "7a4ffa155629424f8cc4fa232d793e1a",
      "be8b1ba665a446a49b30e6475ff8fd9a",
      "3afa95d3166245cd908c210a8f37b7fa",
      "87526faef7674bfd9428a7908c979c1f",
      "d4a6bf8b129d4d4190942686d3bd65df",
      "290c63dbc9e04f4a93ada376c2fb3b0f",
      "a8a224d6e79e4c6e847a0067c385c4c3",
      "a184e52d73af4681bce684a250a108aa",
      "96b57866838e45e68a1adeb2809d9676",
      "9442e3fea9ee44fa9bf0029196fbe889",
      "1b03b9cf2ac747419e8b429e8d8a1b47",
      "64163b76e6b543818c8e1a2b32c12589",
      "b498357c8f1a4147945683040f258491",
      "3abe9952efb2442db2a403922ef4a1cf",
      "4776463baf814420917f890248cb5795",
      "9d7d75aaf07d4146beed734f9fae65d7"
     ]
    },
    "id": "UJnCapHjE746",
    "outputId": "80a62bd4-968b-430a-f9db-170fbc07f932"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Use batched=True to activate fast multithreading!\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZP7C2G-E746",
    "outputId": "92e4fb25-276b-44d8-9e7b-23341166c088"
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUTZmndPwhVU"
   },
   "source": [
    "**✏️ Try it out!** Some Transformer models, like **BigBird** and **Longformer**, have a much longer context length than BERT and other early Transformer models. Instantiate the tokenizer for one of these checkpoints and verify that the model_max_length agrees with what’s quoted on its model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "090a55723a9c4b1abc674d1413db21c0",
      "bca59cd1b61b401ea5a87b7646eff9e1",
      "aaec4d08c93243b39162659eeeccf314",
      "e47d153f481543058d8b821059eb6daf",
      "6e586027210b4a84af1340f3b2a091c6",
      "a6350f1d19eb40f6800573b97db3868c",
      "6394566aa3574a4895feebf7f0203aef",
      "d756dc2b17ad4cf6aac4ee0a4c103b0c",
      "27a752632c8241eda854f9e061776480",
      "6b8c79cc2a9948299c3d3bb8cb222a22",
      "bcdc94cfdf1f48ce85903ca111327cb3",
      "495eb82f7b8d43f4b586debcf08ff216",
      "4733466269d6499d9514f8e47bcb1d2d",
      "74eb51738eff49e1a51cdff74f05d950",
      "d14c6227911c4826b84ac87a6b6e7763",
      "d730d1dbb9754facaf6577761bf1ce43",
      "3e2c328f550e464fa31f0fafac5df545",
      "69e8879d2c1748d083a09f588b637fda",
      "2f0b0cf72ddf43429761806e9a30342c",
      "9b47e2f969b84ddb9016b2a4eb4c69fd",
      "cda5a48482b54c8cb67f98ecda9423eb",
      "fd3d8fea83944215b11ba674dda34a8e",
      "402e917bfbe243888f7a87bb8e5b792d",
      "426c945be7cc4689b0e7ecbb5c864fa0",
      "c07ef30eb7114592afa7a3e23282b9ed",
      "967ffc5ae12944aea1f39499681aec9e",
      "9b981534de164d0ba895da86f197be33",
      "a588b9e843274d2598a5a23e96bd964e",
      "d638ddceafbd429f9427ec541a823f56",
      "5f110ed5c7cf4dabb0af0a4fa087c1b2",
      "020f271875a642dcb1aa0253c9566ae7",
      "e758fcf18f2348d49cbbd57778905405",
      "ae332a5f080d44b7b9137956201fb71f",
      "fd6f6164b15f4a809b54d89c94c4d4e3",
      "b4ba219221a94a2eaf48d2d295629209",
      "21f2031c1e124ef78db6dbfbbcdda0c6",
      "deccb368b09a425998311b9717e4801c",
      "9a4d6b03338f47cab2ca077c99816847",
      "adcfff166f25482c82ee82a77a5d2358",
      "9b07e1b3a5d34de5b87054cec7f2de03",
      "0e7573261ee14acda3ed26df323c2ba6",
      "405dfeb266b741b39b22db2849fd3c1d",
      "6aa139a2c3614a76884f0be5ceb8274e",
      "82e0e15097a34edbab681e57a7db4d1b"
     ]
    },
    "id": "tUOds-18wkgB",
    "outputId": "ba2aa877-465e-4094-fe5f-2518a4ff4766"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "\n",
    "tokenizer_bigbird = AutoTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "tokenizer_bigbird.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBh_Auf9E747"
   },
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75GOD0nBE747",
    "outputId": "4f931519-9e53-458f-f639-b1e5e0e31ad1"
   },
   "outputs": [],
   "source": [
    "# Slicing produces a list of lists for each feature\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZV0r5755E747",
    "outputId": "857bf275-41f6-42be-ec53-9fd8038a8410"
   },
   "outputs": [],
   "source": [
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\"'>>> Concatenated reviews length: {total_length}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cWbtlbCE748",
    "outputId": "8bbe47e3-b55a-4dcf-9634-2e93b94c942c"
   },
   "outputs": [],
   "source": [
    "chunks = {\n",
    "    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\"'>>> Chunk length: {len(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ow_kGnm0yC0B"
   },
   "source": [
    "1.   Drop the last chunk if it’s smaller than chunk_size.\n",
    "2.   Pad the last chunk until its length equals chunk_size.\n",
    "\n",
    "We’ll take the first approach here, so let’s wrap all of the above logic in a single function that we can apply to our tokenized datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVSRJYqsE749"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356,
     "referenced_widgets": [
      "32bd1ebd0091461b9cbb6d7fa899f7a8",
      "b5a4bba42c4c4a22aadfdfbbe278740d",
      "703a8018da6644e693cd5b714d919427",
      "78b502d50d5f4062b540201053f91500",
      "38bdd4a87b38472c98eb32c7ba4c3552",
      "f6b98e040e7743028f359177b0ea6095",
      "0663a09e2ff342309f0fd6504dbdd1d3",
      "e4bb6f83ebbc447e83153c1420c06d7f",
      "392898fa27b74224a5b043419c1c805f",
      "8b9e20925ba64fda9a693e751f5d7f13",
      "56b1c5b7a4f744058e1077bdbd6fce5b",
      "34391ad037a74010a8109194818403c3",
      "f53c78276cac4b859e418173779d7dad",
      "6df645944da546d2b7f75b307fddf839",
      "116241a43ac94e44a0c84eb5dd8b1300",
      "7087e02709e845cb81342ce74c910668",
      "67ccd59b2d3e4dd38e8f09c1141924df",
      "353894adbd544d428e571b54af6befbf",
      "a2113935286b4fbc88ff93933a683dbe",
      "633871e14bf24bc59cab554dc146b2a5",
      "1ea54505baa841e4ac57da6db6268dbe",
      "a283db570973439798097a8642d24830",
      "bb39004dc0514413ba166e62a9675a1c",
      "561888d3c9774e9893d97667a4466a6f",
      "0ca6124101004aae9b5c55a129ea3f84",
      "ad670e1c9dfd4774a7160f0841c0827b",
      "10d8b9b29481482f80791e8772b889f3",
      "d5c4f2358b6d4c53a27b380140ec740a",
      "75c638c4ac8341c19c7af7160b34f366",
      "28c886d271384e6da3a729f591f57c55",
      "c3c241c6bcd648b7876e57e56e8e68c5",
      "3a57e20d1d8c487786225ebfe9bb4733",
      "9c1ad5c7d116433d8c4d5e6d26e1d50a"
     ]
    },
    "id": "xkWo1FdXE749",
    "outputId": "e58242fe-95e1-4a54-f8bf-77a6094ff975"
   },
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
    "lm_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "KbyOwHc1E74-",
    "outputId": "9530a079-6294-420d-9854-d85aafe004c7"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "pO_7qZF7zL4Q",
    "outputId": "3c2a3de5-cd2b-41ea-d6e9-f65177cc0915"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRehLPwazWmx"
   },
   "source": [
    "# Fine-tuning DistilBERT with the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JujA73OE74-"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpuEiEIwz8NB"
   },
   "source": [
    "To see how the random masking works, let’s feed a few examples to the data collator. Since it expects a list of dicts, where each dict represents a single chunk of contiguous text, we first iterate over the dataset before feeding the batch to the collator. We remove the \"word_ids\" key for this data collator as it does not expect it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOHGRiwOE74-",
    "outputId": "c3294fe9-aaba-4de8-cdb7-7b5fb8ca24a9"
   },
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMQ-WfYP0R7B"
   },
   "source": [
    "**✏️ Try it out**! Run the code snippet above several times to see the random masking happen in front of your very eyes! Also replace the tokenizer.decode() method with tokenizer.convert_ids_to_tokens() to see that sometimes a single token from a given word is masked, and not the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2C6LXcZn0Tn5",
    "outputId": "7f513455-0d5e-41f0-cafb-e4bee90520f1"
   },
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnk_UBE7E74_"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers.data.data_collator import tf_default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "\n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "\n",
    "        # Randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "        feature[\"labels\"] = new_labels\n",
    "\n",
    "    return tf_default_data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PvyNaq_E74_",
    "outputId": "1d3b434b-c8e6-4fee-f7c5-84a71913f551"
   },
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbQZQFFk1bIx"
   },
   "source": [
    "**✏️ Try it out!** Run the code snippet above several times to see the random masking happen in front of your very eyes! Also replace the tokenizer.decode() method with tokenizer.convert_ids_to_tokens() to see that the tokens from a given word are always masked together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kF4WQAyu1cvz",
    "outputId": "56c146be-8068-423a-fbd6-b2f313b38b7a"
   },
   "outputs": [],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n'>>> {tokenizer.convert_ids_to_tokens(chunk)}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjNMiJ4OE74_",
    "outputId": "346d9508-ba7c-419e-b89e-9b133db9350c"
   },
   "outputs": [],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n",
    "    train_size=train_size, test_size=test_size, seed=42\n",
    ")\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "43bbe7cbeaba4ba3916cabee92f5bf07",
      "0550151e73664701aa14522003674f6a",
      "517557cb6a0e44bc9a153cf4d9783517",
      "0970df4b12064619bb47eee2a543898a",
      "274439fcdad841f5a1df4f54e43c6010",
      "27dc67347d694e45989f1f37e4f1d116",
      "d591de27fc214a4e87bba58a55661938",
      "00fac9cd97b944b0a77d2a56b99e0f0b",
      "614c1a39fe8f4844bfee762b7bfef682",
      "8193d876948b4c98ae884cdbc5295f62",
      "ffee2031ed824bcfba491dfff8872e41",
      "b032db3dd380463d8ff7e5250815fb5d",
      "a82be48c97e0445aa6d8a688ebdc69e3",
      "81e18b44d4314a3db49a09caf820c35a",
      "f37da37de52b4be7a36b9216e4b09991",
      "7eca995efb324b48bd16d91926b6e1b8",
      "6b346e7be01840aea307eafd16feef46",
      "241fef954c0d49d9a8823df36ace9d9c",
      "64eb289e09aa49adb3c54221235bc32e",
      "b3f4ebe9d82343b9b71054e87e102ed0"
     ]
    },
    "id": "ZkK0vP3oE75A",
    "outputId": "3062d958-e66e-4f65-b847-f993ce6922d5"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai5o2qCi4Ufp"
   },
   "source": [
    "Once we’re logged in, we can create our tf.data datasets. To do so, we’ll use the prepare_tf_dataset() method, which uses our model to automatically infer which columns should go into the dataset. If you want to control exactly which columns to use, you can use the Dataset.to_tf_dataset() method instead. To keep things simple, we’ll just use the standard data collator here, but you can also try the whole word masking collator and compare the results as an exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6EvjPAHE75A"
   },
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "tf_eval_dataset = model.prepare_tf_dataset(\n",
    "    downsampled_dataset[\"test\"],\n",
    "    collate_fn=data_collator,\n",
    "    shuffle=False,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x88tes-xE75B",
    "outputId": "58095c48-e665-45d0-8821-edfe0ab3dcf7"
   },
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_steps = len(tf_train_dataset)\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    ")\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "callback = PushToHubCallback(\n",
    "    output_dir=f\"{model_name}-fineTuned-imdb\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rUXujDM5ayq"
   },
   "source": [
    "We’re now ready to run model.fit() — but before doing so let’s briefly look at perplexity, which is a common metric to evaluate the performance of language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br6nKspX5dRA"
   },
   "source": [
    "# Perplexity for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPZNIZfeE75B",
    "outputId": "16c6b5fb-34db-4982-bc4d-78a0517434c9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtsl7PS363Wi"
   },
   "source": [
    "A lower perplexity score means a better language model, and we can see here that our starting model has a somewhat large value. Let’s see if we can lower it by fine-tuning! To do that, we first run the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tber_AkQE75B",
    "outputId": "b3b68734-cb5d-40af-a6e8-2c1c5d52fc1c"
   },
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hx8fzm8E75C",
    "outputId": "2424593d-1313-4825-e700-a58fc0700307"
   },
   "outputs": [],
   "source": [
    "eval_loss = model.evaluate(tf_eval_dataset)\n",
    "print(f\"Perplexity: {math.exp(eval_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQ-utuB0IHrQ"
   },
   "source": [
    "# Using our fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "7019b3c405394c268b4eb895c06510ba",
      "590c9f276e424411bc530c91eaead58b",
      "f10ec6a524f1449492706b05ad2e1936",
      "a9efa3b6f6f74fe39fbef59fb79fedd6",
      "9b20a4cf07084126b5b4905178c05271",
      "d4bd114bba1c4648a4788581a37f989f",
      "f21a37e33ad648e7a0d5b1c42803786a",
      "a8154adaeacc4eefb69f169c98cca362",
      "b6429a9a553946dfa08c9918b4698ea2",
      "70e04d8d5ec2480ebfe118faf8e3405f",
      "ae6b9df071e145658b86cbbf0762a1af"
     ]
    },
    "id": "4g5ttGq3E75C",
    "outputId": "af01c247-55a9-4b29-f96a-041cfd55f7d6"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=\"Mhammad2023/distilbert-base-uncased-fineTuned-imdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBTmhNP6FK--",
    "outputId": "6b205791-a99a-4f71-86cd-1b863d18afa6"
   },
   "outputs": [],
   "source": [
    "preds = mask_filler(text)\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "name": "Fine-tuning a masked language model (TensorFlow)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
