# Hands On NLP Projects using [Hugging Face](https://huggingface.co/)

Welcome to my collection of hands-on Natural Language Processing (NLP) projects. Each project in this repository is implemented using Python notebooks, covering a variety of NLP tasks such as text classification, masked language modeling, question answering, and more. The models trained in these projects are also available on my [Hugging Face profile](https://huggingface.co/Mhammad2023).

---

## Repository Structure

Each notebook in this repository corresponds to a specific NLP task or project. Below is a list of the projects along with brief descriptions and links to the associated Hugging Face models.

---

### 1. `Token_classification_(TensorFlow).ipynb`
**Project**: Named Entity Recognition on the CoNLL-2003 dataset using a fine-tuned BERT model with TensorFlow.  
**Task**: Token classification (PER, LOC, ORG, MISC).  
**Model**: [`bert-finetuned-ner`](https://huggingface.co/Mhammad2023/bert-finetuned-ner)  
**Notebook**: [`Token_classification_(TensorFlow).ipynb`](./Token_classification_(TensorFlow).ipynb)

---

### 2. Fine-Tuning a Masked Language Model (MLM)

**Project**: Fine-tuning `distilbert-base-uncased` on a subset of the IMDb corpus using the Masked Language Modeling (MLM) objective. This project also demonstrates the use of Whole Word Masking (WWM) during training.
**Task**: Language modeling â€” predict masked tokens in input text.  
**Model**: [`distilbert-base-uncased-fineTuned-imdb`](https://huggingface.co/Mhammad2023/distilbert-base-uncased-fineTuned-imdb)
**Notebook**: [`Fine_tuning_a_masked_language_model_(TensorFlow).ipynb`](./Fine_tuning_a_masked_language_model_(TensorFlow).ipynb)  
**Highlight**: Uses `whole_word_masking_data_collator` to selectively mask full words instead of random tokens, which improves generalization.  

---


