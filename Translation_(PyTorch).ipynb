{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3zrul9i69wW"
   },
   "source": [
    "# Translation (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52OPkot069wX"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4GWxxVik69wY",
    "outputId": "08a86051-0398-4734-ca85-3eb953f54516"
   },
   "outputs": [],
   "source": [
    "!pip -q install datasets evaluate transformers[sentencepiece]\n",
    "!pip -q install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "!pip -q install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs\n",
    "!pip -q install --upgrade fsspec\n",
    "!pip -q install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAUoIKl269wY"
   },
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu0LkYj469wZ"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"your email\"\n",
    "!git config --global user.name \"your name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8So83bBP69wZ"
   },
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "691ca04e209f4739b7f7d87885866caf",
      "e12599f0fa984692bc49c93890e5bd5a",
      "be0df1b7c1c242859a933345d6110883",
      "a2a864aee9cf48938f932a232e22caf8",
      "041c7dd00149476e86b528504e768bfe",
      "5a9483d0da7542b48bc0f2159987c15b",
      "c814a34ae186440eae9a7e2458ffe464",
      "a19f3023094542a0a4af685dbbaf1615",
      "93b4ef80ed7243578ee3acc643d6d9fa",
      "d260000454a64448a290845d9586c7ff",
      "5a283bcaad8d4bf7a9bc4efbd4526057",
      "b2f28bb1d8594d199753267279ee577a",
      "ed61b92d80064a7bbc49639bb142a278",
      "a5ffc07508b641cb98573afdebca52be",
      "154d7da8e1e14711891f1c6040232022",
      "18b8fefb0a664a9590847d196858727f",
      "5af4ea3a15e24431b0c374a532dffd8e",
      "f7d45690277b4538978c4c56d7821e0d",
      "131d6aed356f4d30ac403bb7e85ebe0b",
      "9b28bddbf0744032bba5be35b0b31d00"
     ]
    },
    "id": "_SdsxsyQ69wZ",
    "outputId": "b384e2e3-b266-46a2-eb90-1558eecf0575"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cZmSCWxSORp"
   },
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klmnsxMnSRsS"
   },
   "source": [
    "## The KDE4 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "9bebcc0ce0bb4d4f827c1766ba90fa48",
      "1995601a863c4193a44de60ef1a7bbfd",
      "178e2af433164bae93f5ecb9b1d7bf45",
      "e245617b6c314c7dadd12c2f489fa843",
      "4af4884bf5b043af80b8fdb412197b5a",
      "2308d09bacbc43bca4981758343670b1",
      "b3342312aa9a4342a2781fbb08f83038",
      "e57214782ecc45beb2a0309d0d9267ae",
      "bc8ecc6d2c9144b884c5a9b8802e5263",
      "133c2fb3b60949f6852a61b6aa95f0b8",
      "33d4218adb2e42328e6e76293180c2dc",
      "8466dd0d951f4f0eaa6e7a440b53bd75",
      "fc91c55a6e594babadef2489637e0f56",
      "fcd5500845e04225b8b767ffbaa85a8e",
      "ff89a21829bb46f680897c8a0db9bb5f",
      "da8e0598cb9143edbc793d10f4ea96a6",
      "8fbe6d62e7dc469097ed65ec8a56b3de",
      "ebb2cfcd9def4f40b5c555fd93a4a99b",
      "bcd52255224b466c9f8d4d7159b33461",
      "b4385c59b071462e85e5d978561f8e0e",
      "377178b72bf74e0b93da68b8a29f6444",
      "ea8423df0de7425f81dcc726869257eb",
      "5ead02ac99a845f79aa15e50bca44bab",
      "25224660af7d48bfb82454da2caa50c0",
      "078269925a9d4fe5bcd877dc5a38b63b",
      "842258641c15464ca804e859dcf6d449",
      "ea69d3b9272244c5a3e872140dd69825",
      "7d86bbbf99ba4565a5f58a04c133c30b",
      "a53da4274bba40668ac193f5e6e91966",
      "1c0129e188cd495f8d41472788a28dbf",
      "edaf13861f224d96b10b149c1078b8de",
      "562f2ef08c5843f4acc49d44bc2547b7",
      "faa9637e8cbb443b820c1e0507cffced",
      "d8e54a3d57794163896af6924c4c839a",
      "7ffbd6f3138b48559bf53c205feeced9",
      "cf9015bafac14403a7683828f53f6545",
      "c9bbc7d2baa44c52bb40c232f3db711c",
      "9c835ea4980a403e8e948a1b72e10e65",
      "c31aaea22f7e491583633793e692ea2d",
      "0c1d1fc68c2a44c6adbdd4077a271546",
      "7bfb25c5677b467e8d28a6ecfeb2c536",
      "8bbd7662c06b487ead9ae54e01e64bba",
      "c7d8a6da0c184cfb848401abb93b1e81",
      "db97b1d218dd4fabaa47747553734a9c"
     ]
    },
    "id": "hWu9sG-k69wZ",
    "outputId": "81e39a70-c1d6-4c9c-8bf0-9863d33130a5"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqWrLvCM69wZ",
    "outputId": "f70563cd-87f0-461e-ed42-08d93d99d68c"
   },
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsRuZGIM69wa",
    "outputId": "28f77fb2-14ff-42ab-ae90-fe4da6eb9375"
   },
   "outputs": [],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Le-589msSX6w"
   },
   "source": [
    "We can rename the \"test\" key to \"validation\" like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VvF3qx469wa"
   },
   "outputs": [],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmPkkk0KvnIr",
    "outputId": "c9b1849a-51c0-4f39-a923-a9ae2e6d7f3b"
   },
   "outputs": [],
   "source": [
    "# split_datasets[\"validation\"] = split_datasets[\"validation\"].shuffle(seed=20).select(range(6000))\n",
    "# split_datasets[\"train\"] = split_datasets[\"train\"].shuffle(seed=20).select(range(20000))\n",
    "# split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DwIARRDD69wa",
    "outputId": "7a615aad-989c-4241-85a4-b262d9067427"
   },
   "outputs": [],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0NMfgaBsO6yB",
    "outputId": "1902b8d3-e3b2-4339-bf36-7f31a0ce2fd8"
   },
   "outputs": [],
   "source": [
    "!pip -q install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308,
     "referenced_widgets": [
      "035e7c8ea4954335b7f867710221d469",
      "41812f236eb148b0a73094277a638924",
      "9e4bf78e39564b1283639c01cf2d5940",
      "b44dc0f6a0704f458d1b6e395eaac1c6",
      "59533bf29e804e0099a69236a022007e",
      "21eaaf48cfcd4daca4839e47182b9ef9",
      "3146e7b8ac7d4a8eb521f6b2082d95bc",
      "ede4592f499944098d50f7effe7c2466",
      "c2d1738b2e064c43a9055eedb4459e41",
      "ac0d565bea1048d3a1a07daf52d83995",
      "bdb5287ac7be448dbf5a2bf2b0626e2c",
      "dbd67c348cb04305967f776ac4ed99ad",
      "bb4f023ef38f4d05b9d806a3ee18e628",
      "21d7e1ec99294f7282946986ecf42ff1",
      "294882aa72774709960711a20ff9d779",
      "c4f83891035a4a96afc06fb8135e27a3",
      "31c7ce69a3e84ac8a8d8da23bd6425e5",
      "fcfb84e19afa463caffa06fac8f06449",
      "c60eed96848d43acb3faaa6af9be0444",
      "bb21234f4a154ac48f09b57e2bc151e7",
      "7cfa3f01c39943c4a300775c1f52b50c",
      "093f02356687442589fd324a26da6e9b",
      "818f1701f0864afd8fd5e0cd0e0e2f86",
      "9ce37d9ef41a4af5b8092bb4eced35ae",
      "b6f365743abc489c970f6d065f11a124",
      "4e34f98828e0424b802ed3d6dac1e357",
      "543b1a40eb594a79b0688facda62c47f",
      "1097209081234ed6a457bbe6fdf024d7",
      "d029ef2908d94ae5b82e30ef9b7521b1",
      "932e542f2d5f4711a3aee6c0b53586a2",
      "0c616c62d87f4b2195ce8c4b6f3e4548",
      "070a9554674b4d4e89b9442dd289bbec",
      "4a1352fa22b5456988a1991b6cda5e93",
      "06c82e83b0104bceb52bded9931ed44b",
      "748db944545348d99c6f52b55ce047ef",
      "94c3b806d9884797894d096ea284734c",
      "4b3c67ab6f304526a6c3a79cd43a5218",
      "b847bb2958994103b59037ecc2d9ce50",
      "ba04182c9c8641a29a36383200dbee72",
      "60ab933c381d4edd808d48061c973e79",
      "2cf17f935bb54375b44fbbe3cc7d618b",
      "0ef1ea4b25514f46a7d125b112affd67",
      "221c6740dcbb43279165bd86708aae70",
      "d752e32b1f4545b1b9dee7169c74b69f",
      "339dac42790c41708e5cc07bef54ba98",
      "f5ad09163b3a4ebcadeb33137580cf93",
      "0422f921e0584a499cfa0547db705fdb",
      "3236fbbbb83948778b4e711cb2f8bfd7",
      "3052514817c8410c96c99a5f32793939",
      "b395fa1673ed4dcbb44d956777232682",
      "8525ff25384e47498c5065218dd9ff32",
      "4c3d3b21a2fa4175ab3dd8cb5bfd2911",
      "4156d232d5ec4780920b14580c0b8c07",
      "944402f44d08483a9a8b7b4accaf3955",
      "219adc59cc2740299d873bcd703e5562",
      "59b940130d6e4931bf8cb1d57179cbeb",
      "7cfc207ddcd44ee1ab36bf00d66f66a8",
      "99d63f9e131d4c6991dddec77e8beb29",
      "26cab6ffae5f4f4b8a741466cc654fbe",
      "f1801616ec944bfba72085ce1fb8670c",
      "c84b4cedb58545f389a441afbc46caa0",
      "341dc5bf75d0495080cb3677981cf09f",
      "01d53d43f2ae42b6b7224643aafaa84c",
      "a27d75983f51413cb22a0a12b103ba79",
      "8f0d878b143d45f6a359b1ec328c7160",
      "1fa1c5f05df142f3a2f0939ee7f1b628",
      "906beb9271a3402a8630f50799af676a",
      "5f834a0e81784a99b8b58b6ab5bbdc8e",
      "11ab1b3e6d4f465cb23bc45c37d9cae6",
      "a01d6caceb444b68abaf589c8c35148e",
      "11eaf54cc6ba488f8d7bcceae57bda0c",
      "1341dde06ee54542961301b2ac01a19f",
      "d06a453fb9d34392a359e717b3dba552",
      "c3e1de49b9294075b39ef5426fe2d2e3",
      "f35be5bb7906492e974eeb22469cac45",
      "dd0ec5c543a749e0838bbbd81d1777b5",
      "44fb95aa260f45adbe4320b1ac773fd9",
      "aa7e467b6d134a6d963360c595f53c85",
      "b8487e538d2f4bf3b671aba5eddbdff2",
      "49ae2e289d754f77ae2969cbd5abf69d",
      "ffb6936a96644f9a9de6e59e2e6caaa6",
      "ab4e20c3eefc405e9b5da2cb2af6c750",
      "e13b2366f3064087a01eaebd3578e598",
      "179ff3d673f14c33a818f51e28ac7c37",
      "41fafe5719fd4aaa98c6390d282c7bbd",
      "d1740b1ad14c4286a014d11c471ff529",
      "e2851aa343384b46a0fb2ee55b0cb34b",
      "2e404232a399492fa5fe72e82acbfa19"
     ]
    },
    "id": "YanCEQuc69wb",
    "outputId": "a13b1287-807d-43f4-f9e0-7b882ae13056"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7ibIZhvPmfB",
    "outputId": "ebf5de24-1e3f-4a54-dfef-d79a7c2d5db4"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uDi5IijsPLEY",
    "outputId": "29b3fe38-38b3-4d52-d004-a493da8a8181"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwDwJh4C69wb",
    "outputId": "00cccd96-f392-4950-cc7c-44542af713f3"
   },
   "outputs": [],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dxv9ymh-69wb",
    "outputId": "d4560b18-89bf-4051-f9f0-9f7055779079"
   },
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lmk30D7ZSenD"
   },
   "source": [
    "✏️ Your turn! Another English word that is often used in French is “email.” Find the first sample in the training dataset that uses this word. How is it translated? How does the pretrained model translate the same English sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1EpBfWC4SfVZ",
    "outputId": "aa8d3d3c-5faf-4252-8d89-59b915d932dc"
   },
   "outputs": [],
   "source": [
    "for i, example in enumerate(split_datasets[\"train\"]):\n",
    "    if \"email\" in example[\"translation\"][\"en\"].lower():\n",
    "        print(f\"Index: {i}\")\n",
    "        print(\"English:\", example[\"translation\"][\"en\"])\n",
    "        print(\"French:\", example[\"translation\"][\"fr\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baFsXij0SjbR",
    "outputId": "1f34b465-b932-4b9e-d361-b2c98dbac5e9"
   },
   "outputs": [],
   "source": [
    "split_datasets[\"train\"][356][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ERqbJZZRSlFg",
    "outputId": "6438f95b-c0a7-4824-f3a3-36f09aeefe99"
   },
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Sends the chart as an email attachment.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2XNheVPSq3J"
   },
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yscd5qXK69wb"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cbKMgCl969wc",
    "outputId": "23a8b626-351f-4f53-96d5-0d04c09dcf05"
   },
   "outputs": [],
   "source": [
    "en_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\n",
    "fr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n",
    "\n",
    "inputs = tokenizer(en_sentence, text_target=fr_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APtsA6H2Sy2w"
   },
   "source": [
    "As we can see, the output contains the input IDs associated with the English sentence, while the IDs associated with the French one are stored in the labels field. If you forget to indicate that you are tokenizing labels, they will be tokenized by the input tokenizer, which in the case of a Marian model is not going to go well at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgBYJ6A9S71x",
    "outputId": "57582aa2-98c0-4313-8957-d0c7f4593900"
   },
   "outputs": [],
   "source": [
    "input_test = tokenizer(en_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(input_test[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQquVLNX69wc",
    "outputId": "5a20216d-2956-4c33-9a2c-14506523f0b7"
   },
   "outputs": [],
   "source": [
    "wrong_targets = tokenizer(fr_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEmO3pKoS-vJ"
   },
   "source": [
    "As we can see, using the English tokenizer to preprocess a French sentence results in a lot more tokens, since the tokenizer doesn’t know any French words (except those that also appear in the English language, like “discussion”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH7hauKb69wc"
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoWAnk2kTBNZ"
   },
   "source": [
    "We don’t pay attention to the attention mask of the targets, as the model won’t expect it. Instead, the labels corresponding to a padding token should be set to -100 so they are ignored in the loss computation. This will be done by our data collator later on since we are applying dynamic padding, but if you use padding here, you should adapt the preprocessing function to set all labels that correspond to the padding token to -100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "8809ae3c03e645c09c3f394e58f2c9d2",
      "32855777a8d94b71bbc6a969436d8506",
      "8d919ebac84a41cb9eafa31ba6a7de0d",
      "284048ffff1a457fb4fc742bcee74d8d",
      "2b93a4e3eddb4806b11a834bfa79d05c",
      "573c039157534dfca221ebf285fb9df3",
      "366dc3c95416451b8f413f672a9e5111",
      "a815f2a6f0284668830badff4feca44f",
      "131bfed4a39d40469e783e715f3dba0f",
      "2094fcb567674fd0b0f205f4f4912dc5",
      "5767cd1212364f0d9774263ed84c6d71",
      "532d902b24fc4a2c95a4150d7c4b14e2",
      "0ffd3a41800d4db783bf6266da3bba02",
      "3f93478bb8ac493d928d1a36fa1eaa01",
      "9f9e34b16c8541218bb224b255ac57ab",
      "fc3a50e56674450996d68c8ad64fb957",
      "8cde2ba3982b41779fb2567b67e25d1b",
      "ab9cbac08aed4a10b8e68c4e46140816",
      "3e90f0ff64d44fc9872156a1cdf00267",
      "c9a070bbc4cd4a8bab4b0571fd3e3cc7",
      "663dd416e8094b1888f0f94981ab3d2c",
      "06923e3998e545879eb098a428f63b60"
     ]
    },
    "id": "-USdu0TW69wc",
    "outputId": "57c09950-7ab6-4444-9b7b-5600114fe2c9"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F02zSo5TFQJ"
   },
   "source": [
    "# Fine-tuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PauE7e3369wc"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-LSMO-4Tj0p"
   },
   "source": [
    "## Data collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUyprNww69wc"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFvttF5U69wc",
    "outputId": "21010440-dd40-4ba9-bcc3-c723a09f3883"
   },
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hLF_F_mN69wc",
    "outputId": "717f1b68-a842-4c37-b55c-d64ea205602c"
   },
   "outputs": [],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZQCSXjp69wd",
    "outputId": "76d20b5a-85aa-4876-eb6d-11a787a06f4e"
   },
   "outputs": [],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZdD6gph69wd",
    "outputId": "319bffc1-1b70-4126-c21f-c243525ed3f7"
   },
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdMXtyvyTof0"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uOWJiayP69wd",
    "outputId": "5cee55f6-86ef-41ae-8ce1-37032a3aac75"
   },
   "outputs": [],
   "source": [
    "!pip -q install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fa361e63242c4c188c237fd4cd81576a",
      "de924e741ab44de2ac6a210b5c977c33",
      "855407c0e8f84c4a8421ad2352b296e1",
      "38dbf863abb74c8eb0515b03d950af81",
      "e35c3a20bc71435088f4638db381e1a4",
      "dd1723b2c2904f6092dedb24ebc1d93f",
      "827f23f1b86b47b5a0c361df75014af2",
      "980dfcb697aa4473a0fb09dadb28b4d0",
      "ecdb28d449a842e2aae03348b32fba12",
      "6342fb318dc841bba81ef36b5b101a67",
      "6e80fa16112d4eb4a7767d7a7d834f5e"
     ]
    },
    "id": "K6pSTJ8569wd",
    "outputId": "7d6e7427-ca6e-4ab7-fc42-f33489415784"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9m2fIMC69wd",
    "outputId": "b00080fb-e673-4397-8f44-84dd26f3e23c"
   },
   "outputs": [],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xBatB4HE69wd",
    "outputId": "359acf35-15e6-4608-9aa8-1614eef9fdce"
   },
   "outputs": [],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMv-b3A569wd",
    "outputId": "96560ca9-3ca2-4add-c86f-69eae423870d"
   },
   "outputs": [],
   "source": [
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3XLX_3uT5zh"
   },
   "source": [
    "The score can go from 0 to 100, and higher is better.\n",
    "\n",
    "To get from the model outputs to texts the metric can use, we will use the tokenizer.batch_decode() method. We just have to clean up all the -100s in the labels (the tokenizer will automatically do the same for the padding token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE4sIGBl69wd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyTzXtzjJpP9"
   },
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_preds):\n",
    "#     preds, labels = eval_preds\n",
    "\n",
    "#     if isinstance(preds, tuple):\n",
    "#         preds = preds[0]\n",
    "\n",
    "#     # Generate predictions with max_length\n",
    "#     generated_tokens = model.generate(\n",
    "#         input_ids=preds,\n",
    "#         max_length=128,  # or your desired value\n",
    "#         num_beams=4\n",
    "#     )\n",
    "\n",
    "#     decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "#     decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "#     decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "#     return {\"bleu\": result[\"score\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfrxDsqoQZzh"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "24e841ddc2ee495cb129e3f43249ffc9",
      "c7cfc0617b834f11950718984b037e51"
     ]
    },
    "id": "33VRnUBp69wd",
    "outputId": "85ad2810-66b4-40d4-dead-997c5a912a2c"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ui2TW4z769we"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"marian-finetuned-KDE4-en-to-fr\",\n",
    "    # evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6blmiqnf69we"
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    # tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "e492e712d2c642539c80b2e0727d8ea6",
      "53894acc07c2482e81a5dabcf78d7eb8",
      "4fd30ee4497b4806b67cd1c985efbc9b",
      "772b37e57ecd456d9717a50a134997d6",
      "3c2ffc281a2c46b494ca41ec4c406c60",
      "989e9cd9c7734f53bb7d44ffdfe9fdc5",
      "bcde6b3d600748778beba34311585ef4",
      "c6f7d07620924122aae7e446429ac39a",
      "ffaa425e25c8419abb9fb861a7b6aab6",
      "76ecc5e6a91c40d2b189ea9fae378263",
      "92333bdf43764e60a47afaf85f35ae5c",
      "64c0b9def3044149a82b1b9b707c42ef",
      "ed79d3cf02fa4132abc49913a4d86b16",
      "e5a1e4532fa74471a7ed3069e62705e5",
      "d059dfe84560479fb1a1e264499058e1",
      "3eb5b2164fcc4b52aa8ded1c3954b30f",
      "10579a12b3894299a84d8f2f1cab1e3c",
      "c2693f96e44d414db2eb4b221c05227c",
      "6ef684376f5e4409932046bb66c0f28a",
      "3797bce33c474c5586f479dca724b7ce",
      "c0acfaa3a1244c44b28d8ea125e4817b",
      "efb039e0904e41d6b5926d0548397899"
     ]
    },
    "id": "PACS7VAMLNKU",
    "outputId": "68ae2074-5d74-4fd2-cb09-ffbe65c86b4f"
   },
   "outputs": [],
   "source": [
    "# from transformers import MarianMTModel, MarianTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# # Load the model and tokenizer from the output directory\n",
    "# model = MarianMTModel.from_pretrained(\"Mhammad2023/marian-finetuned-kde4-en-to-fr\")\n",
    "# # tokenizer = MarianTokenizer.from_pretrained(\"./marian-finetuned-KDE4-en-to-fr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7fO8kMtLbaj"
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./Mhammad2023/marian-finetuned-kde4-en-to-fr\",\n",
    "#     per_device_eval_batch_size=64,\n",
    "#     do_train=False,\n",
    "#     do_eval=True\n",
    "# )\n",
    "\n",
    "# from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "uwhTopg469we",
    "outputId": "97f58344-abad-4fc8-d087-474503418f85"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OQbwBj1469we",
    "outputId": "c4344300-1ab2-4670-f6e2-2d25b1382499"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "Wtv0zrKbXCEZ",
    "outputId": "f752c398-34f4-48dc-80ca-bd9d56c90e5c"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEkj7zbZ69we",
    "outputId": "f019818c-ee87-4bc2-e6c7-e04aa8fc9e96"
   },
   "outputs": [],
   "source": [
    "# trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "bb83dd859a204145a42c491cf9b660d5",
      "915224fc6ebc43db9d5d87fde44552c7",
      "fa4b4a58f20642958c85f12a2f6ca131",
      "d6fa4fd3bf244160880cabb576328e47",
      "3f7fd75030a644d995d46011bf674023",
      "3b9817bae8c7414b8b6c178ee9d0cc08",
      "61111257b9c446b19e79e85a04cae8cf",
      "74f8dee254ba4c0c943cfb903d5051a8",
      "ed47c0a18701487caaed044a6b2f21bc",
      "9193b8fb955e4558865eac1ac4c571ef",
      "f76fbeeb861349ce80128d4c4aa4cd1e",
      "9b2ded7e68934845b10f35c013547383",
      "9567c23c8fb74621abccaeb77ebaa056",
      "3b1b7617ed3e4495b9d6f36dd0f57a20",
      "2661f5d11f564e7b97d18c022f08ab97",
      "f6f5f54570114f8bba58ea78892aac02",
      "250219851c944cb7965cac08af452094",
      "b6a3e11c27b546f2afdff726388f5ad9",
      "2e1cb7d488374a608b4a6b293e2bca54",
      "6007dba46e1c448fab24c5b232c8c4be",
      "a5a21a99def74bdda79137bf6a9b96e0",
      "897d3f7356244257ba56b0f03b5bdda6",
      "944b80b23c294712aa06858e28eae217",
      "253e4748dae74a63a2e05e32b0bfa10c",
      "2bda51ea0e7841dbae85a55367e2b081",
      "8da898722ed54991adcc84a355b1f0de",
      "1b10d0256b2b4aa7b4d3d0e3eaca92cc",
      "f32c909c1cec419aa039b49cfba6c562",
      "c5090644d1f94a229b2f078847d7103c",
      "3e6f55af3b0f41fc94c5a3aa11712103",
      "dafefc36172746299aa47ca121c5c45b",
      "e283a80d0b7a41869bb6f5c09d45cfef",
      "1b15041c5f0e44b5bdaf92a2bad8fe52",
      "65495c130daa422380424d3be43f1e5b",
      "0f4d4e2ddfc84569a86d2d24cdd47bb8",
      "96aadf267e9a495a812778d06cd7db5f",
      "7a18dbb59c334b46a6f5edc7f6f16cf4",
      "bb4fa1ca02564969a52c5a1b893bffba",
      "7ecc3e800905414283a97d90a53d83b3",
      "548d191994474d4c8a462b5bdef46b6e",
      "76ca9090fc6a4f599fdb12619d706eb1",
      "f4b2ccd375f44beaabac621bcbef4105",
      "f6485e970a604a66b6431a3c42998004",
      "63763762a2fb49bfb10fb7545db48698"
     ]
    },
    "id": "6m_RxnI469we",
    "outputId": "f4e02ef9-1017-4cd8-d07e-b98407106a23"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtSpRWlntGRA"
   },
   "source": [
    "If you want to dive a bit more deeply into the training loop, we will now show you how to do the same thing using 🤗 Accelerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8osXD4NsUrZ"
   },
   "source": [
    "# A custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-tt-gYjsY7A"
   },
   "source": [
    "## Preparing everything for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYhBBzPw69wi"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3mRAV98ssxo",
    "outputId": "cc93e9eb-3e8c-42b4-894c-6ae92dcba461"
   },
   "outputs": [],
   "source": [
    "eval_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4m-nBSq3syPB"
   },
   "source": [
    "Next we reinstantiate our model, to make sure we’re not continuing the fine-tuning from before but starting from the pretrained model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gz92K9Oe69wi"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuW5dzVx69wi"
   },
   "outputs": [],
   "source": [
    "# from transformers import AdamW\n",
    "\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaYUz87KeD6N"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHuoUDsL69wj"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xO25UI-V69wj"
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "0UgWucA669wj",
    "outputId": "31a70fae-9c1d-4abd-f799-ae3ca54f1f79"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k4f_Cqrp69wj",
    "outputId": "f2f9f3c3-3e52-451e-ff0a-75f2fd21b25a"
   },
   "outputs": [],
   "source": [
    "output_dir = \"Mhammad2023/marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Meq9oeIVt5OR"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isK59QIm69wj"
   },
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "36b01755407c4962a3bee4ea862a67f2",
      "399289b2518a458a824eeb1c5548ea0d",
      "0e4acea558674ef5aef28c9c12918b01",
      "21b6619ae3104709ba1de3f6573dbba6",
      "2648e7a48f8b4e7283131d8896d4561a",
      "9fcc7eca2686497ca38b82dede1790ed",
      "ace23e0c28bc4c3395eb326a5c330170",
      "10edc5af9f53419da0f22e8f0ed59132",
      "d6c5d35902304709873f9231899530db",
      "99b96f0e8a3c4410919b7df8ceeecefa",
      "7c9db150f836416ca100aa10eed82d55"
     ]
    },
    "id": "Hwhmr8Zg69wj",
    "outputId": "d7044cc8-f131-4b4e-d177-8b4664bb8ce5"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZg8bacMuuRQ"
   },
   "source": [
    "# Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYn7uDOk69wj",
    "outputId": "fde8b27b-e0a3-41bb-a3c9-01ac750f66ca"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVWshFiMgsev"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"marian-finetuned-kde4-en-to-fr-accelerater\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "translator(\"Default to expanded threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1csj7EkjguCW"
   },
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFSi_9k_69wj",
    "outputId": "57e765ad-e7dc-49c9-8f2b-c6f0d246ff9b"
   },
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wot5k3DFuxmB"
   },
   "source": [
    "✏️ Your turn! What does the model return on the sample with the word “email” you identified earlier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BrVdJXeRuyhx"
   },
   "outputs": [],
   "source": [
    "# for i, example in enumerate(split_datasets[\"train\"]):\n",
    "#     if \"email\" in example[\"translation\"][\"en\"].lower():\n",
    "#         print(f\"Index: {i}\")\n",
    "#         print(\"English:\", example[\"translation\"][\"en\"])\n",
    "#         print(\"French:\", example[\"translation\"][\"fr\"])\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoOSZB8Uu7Q4"
   },
   "outputs": [],
   "source": [
    "# split_datasets[\"train\"][356][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0qOWtPbUu9go"
   },
   "outputs": [],
   "source": [
    "translator(\n",
    "    \"Sends the chart as an email attachment.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "USdSwKPUgvdB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
