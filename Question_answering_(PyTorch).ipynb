{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-yWfUGc9lAt"
   },
   "source": [
    "# Question answering (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lT3bpiyj9lA2"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1EGf4aG9lA4",
    "outputId": "b5b5ce77-8366-45e3-f158-335c6c7a804c"
   },
   "outputs": [],
   "source": [
    "!pip -q install datasets evaluate transformers[sentencepiece]\n",
    "!pip -q install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "!pip -q install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs\n",
    "!pip -q install --upgrade fsspec\n",
    "!pip -q install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTjmUG9y9lA_"
   },
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o8bk9tB9lBA"
   },
   "outputs": [],
   "source": [
    "!git config --global user.email \"your email\"\n",
    "!git config --global user.name \"your name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8L2X0iaz9lBC"
   },
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "7c0c4d555d7c4175af9d961e832452ba",
      "a9c43bf36fa141bc862c06ab0b09f6ef",
      "47da3c92c3ac479790c9e91126b8404f",
      "7b989f5c24694e3fa0d57de9d454f487",
      "5f92861c92344f87b02d49e88a7dd380",
      "244e26bc0849485aa2230dd44c359300",
      "d9727de5237b45778c6c3d175f0ea324",
      "009a28663d1c4edb9e7ccfb0a01efec7",
      "fbc41ad1d5e0418a8aa8d4afbeeff1d2",
      "d6e1b538f0204adfa2c7ee1ab8210f92",
      "9513dd81e5d245288c6d198da8b9dec7",
      "b9c3767c48994fbf95c512db899abff3",
      "25879d382225449b890a80e2267d6589",
      "3761e45fe99246b7add55123c33b61d0",
      "42a6a39367b343ada1ff4827df0ce13e",
      "eee5c9720fca4dee8a03ab4c76d99e4f",
      "58a47f302d594e468b3c46fc3bb00f05",
      "4dffc1bb40f24c95aac60fee633fbf68",
      "732cfa68653446939078606762493e18",
      "8c55afcb12d5440fb9b04fc824a87873"
     ]
    },
    "id": "pt4r1SQR9lBE",
    "outputId": "249537a7-49eb-4307-e650-7f9bf8ccad9d"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-0abIZLezZS"
   },
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JESVxGTIfOb9"
   },
   "source": [
    "The dataset that is used the most as an academic benchmark for extractive question answering is SQuAD, so that‚Äôs the one we‚Äôll use here. There is also a harder SQuAD v2 benchmark, which includes questions that don‚Äôt have an answer. As long as your own dataset contains a column for contexts, a column for questions, and a column for answers, you should be able to adapt the steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuPG-E2Pe19y"
   },
   "source": [
    "## The SQuAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "189d85c24f364ddd99b50c252f224975",
      "6eee7704cfeb48fe88ed330afd4d6a4d",
      "9661f005dd9746ac866c90e0b2e524a4",
      "bc5d4b529935430bacc5243a705673c2",
      "242a2b3d898e4160958d1e7494f8b223",
      "4d4838a77d4747a8be6490b3c0778fd8",
      "cbc0376af0374d00ac4cad328dfb3973",
      "4c2795e82ce04eaebcb9bfb9d4047b88",
      "587b351a1e794ee09ff9d68d77b7db1e",
      "236aa11b56554fbcac4840cd60141ec8",
      "a8b8f6a516604885bf802786c91d776c",
      "085a6bd610494285a074515624f45495",
      "e2ea14f21eed4e108c3dc73920d2f6bb",
      "098981b0ce144ff2a01b4c3a9f497d77",
      "67d94533544f48ab9918d4ad6ffd3883",
      "ff022a2ef4054c91bb4eee20f5413b60",
      "cfbe9d8ce9ed49ab80ad10befa027f9d",
      "e916d9a8d95f4a489a5c709be07f7811",
      "7f55b1952ff44e6598f5d606e54149f3",
      "99ed30af766e43c597f7167b27cfff00",
      "abfba2aea732496e98f4015167505597",
      "a87ae5be7c8740c5a756a7d26f8fd02d",
      "20109da478b54a0184880c69b085588c",
      "8b1b8e6e4d96440a9057ea601e69b66c",
      "8546524f496c4223bbac5e5e5b87040d",
      "0a07102e78994c93b09b0b30d69ca3b6",
      "542cba20db9d4105ae51a204ca0725b3",
      "1268cbbcc2d74137b89aa4940ec51f07",
      "6534312a970b4052ab3cb08acebf37dc",
      "b43046272c674dfe80811d1c0bebc7ee",
      "54fc64b2ce4f4b80a37964fdc566c72a",
      "3520be407d14453693864f294a516dd5",
      "6ae28f4d65ea4a1b910a1b4e85ecf9ad",
      "e687cb1b7d2149789636bd89e23402c4",
      "bc101e98f9d54894af275cdc1e84d2ad",
      "aa9ba3d701c34cdd8bc388121562edbc",
      "477021f861574df18daf5df2e19dec1c",
      "2133ce59f5bf4e8f8cc83488d9924732",
      "c928e7e2bbd3413a858d8aad4cd63431",
      "e422e68c531e47bfa2bd3992763ffd28",
      "431cdace379d41ea8ae0ac4520f2ba70",
      "4b316803c6ee4427902df72be4fc8319",
      "d5941f7c901c40108272ca23b16d9bdb",
      "69854af02ec44373baacc6f5da5bbb6d",
      "2557615b53dd40b4a7ec719bd10adb2d",
      "0cd44b92e70e4419ae0719f0066d3931",
      "e4b4e717f81346d8ab59dc9968de3ef8",
      "85baa4de6565468c9a1b653a21072280",
      "10d8dc53143d417484f067696d33f2e6",
      "7931eca343f3429baa5478c21b4dbe53",
      "b1f3093918464bc2a1229785f0880798",
      "b14e2aab4c66434f8f2940217e341172",
      "3fcd07bee3aa43e59eae2838731c6028",
      "439b1f3be2334aec8b8e47ddd850eb0c",
      "a50e0abc143943a19c76d87e4439a38e"
     ]
    },
    "id": "gnSOqUfn9lBF",
    "outputId": "24987858-c08f-494a-8b37-52e343d8f379"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ilAZlf6K9lBG",
    "outputId": "b9b2e416-4ba1-4f01-8385-edbd9322b584"
   },
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zf-Uhl429lBL",
    "outputId": "1fa49ac9-f7db-4097-b738-a07709a99bdb"
   },
   "outputs": [],
   "source": [
    "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
    "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
    "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgQersDrgStC"
   },
   "source": [
    "The context and question fields are very straightforward to use. The answers field is a bit trickier as it comports a dictionary with two fields that are both lists. This is the format that will be expected by the squad metric during evaluation; if you are using your own data, you don‚Äôt necessarily need to worry about putting the answers in the same format. The text field is rather obvious, and the answer_start field contains the starting character index of each answer in the context.\n",
    "\n",
    "During training, there is only one possible answer. We can double-check this by using the Dataset.filter() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "41ac8806febb438292ac3622df503528",
      "263c8a3371f94f02aecdcacb03138f4b",
      "6733cafcf0ab4af7937e3b137c5dd70a",
      "2e84a2a9301c454e83b2d12c8931b7fd",
      "1d94611b08094755ae7eee9c62be7aea",
      "17cf79962d47496581ad8d95cc9f3bd0",
      "8ed743ba3c9f47258892614610f5123c",
      "62214386239e435297b03686f5f6aac7",
      "0a3ad9f5d5274afd98c534c284e25be3",
      "0247b2befbc74a9394236b360c391729",
      "9b6eaad641d24049a75e84d4bd6944ef"
     ]
    },
    "id": "MSH1d_vD9lBN",
    "outputId": "8531d958-2652-46a0-a363-1d4a22b7045d"
   },
   "outputs": [],
   "source": [
    "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FaBEt8Si9lBP",
    "outputId": "f5caa50b-c115-4760-e78c-df3db13eb848"
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[\"validation\"][0][\"answers\"])\n",
    "print(raw_datasets[\"validation\"][2][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaP2tAuU9lBQ",
    "outputId": "757160ee-d317-4fb3-eb4b-4686999ad6b7"
   },
   "outputs": [],
   "source": [
    "print(raw_datasets[\"validation\"][2][\"context\"])\n",
    "print(raw_datasets[\"validation\"][2][\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSVe5pRYhOod"
   },
   "source": [
    "## Processing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_Mjgr7cjHiT"
   },
   "source": [
    "Let‚Äôs start with preprocessing the training data. The hard part will be to generate labels for the question‚Äôs answer, which will be the start and end positions of the tokens corresponding to the answer inside the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "961fee2ac5e3474c82ce6a2f881cc4ee",
      "895b76bcaaf04084a858d54599342a92",
      "f11c90e0027c45a79088712f9b0923bd",
      "4909852f5e344fcbb662f61b7994788c",
      "4a5f4fb5d7fc471f8c3a4dc6e55f7534",
      "7d61f84432bc41ea8016bc9bb37ade01",
      "4115f60de06242f8ab30c160451ffe62",
      "f611a489570e4b18bea8710428bd2fe0",
      "9ce3c0e6ef484b6294e731c5530a4d9b",
      "5ef8ec6a63de4b5d9af39762d3f64aab",
      "4c31b1b714b44933982ae368791c9828",
      "9a7be49579224b88b9c268e6d1b551eb",
      "f39ea3e13f43490daaa10847426e1b57",
      "b0b84bc9ec36489980810214e8c77399",
      "399ce8a053484700a5f9879c3bbb2e3e",
      "4195833c8ae24ca5b131939c5e643cfc",
      "2dc754fc63a64e2fb10d800b337f6882",
      "ae4c72c9e60e4a3a95e458132d16c00f",
      "8cccb096b4ba4cc080f5a98b0bc38ea4",
      "dab80d5fb3554a2fa78db92499a59f07",
      "f81408755d684a7fbfb62d0e0ae86489",
      "a5e6ec52654342f1a220e6c9df9aee05",
      "d9f5ea21505743ab8fab1901f2ca1e38",
      "5fb09942511d4c1a9ac9c1074f840086",
      "0e30e31415e64ce89f9a30fe94d3372c",
      "ebad999aaa6f412eb3256c1036ac69f6",
      "9503eb4f04504b20a0e8c134150ad584",
      "b6e8226d73a84029b68e30a6e6cca1e2",
      "ef1f56833f154d5ea9b43ffc951ae991",
      "af0298fd18024fab86f0704e1652d293",
      "a6cb6831beab47fd9ba69ec40220951a",
      "15a642dc52f248da91f11bb0f67897bc",
      "d814d429a0654556aae7ef1e49ab37e5",
      "d2d60af35473478e90db17081445ac0f",
      "bc8224bb08e84f5d81cd569ecddcffdc",
      "cdcc181aeccc4645b435bf450369af3f",
      "f18e4d757a9c46508b6182a2397a0b80",
      "b17a4be3bdcc4afe9edb9aca50e93fb3",
      "b30131ad217a403789555c219ca41658",
      "0298112767b14263905f90f6ff464c66",
      "b2d0d6d4fba143989a0a7d0151c0c897",
      "a1bcf017d24b4fcd92e6460ed066ca23",
      "8936043c814e4bd582fcef5fca22b23e",
      "860e6db4f6dc4ad093200193a531ac24"
     ]
    },
    "id": "7K2HSZZs9lBS",
    "outputId": "628a1c7f-c269-49ce-fa7c-8c2b0a271a46"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fsLEyWr99lBT",
    "outputId": "1293289c-949f-4322-a59b-8407121c4a15"
   },
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOIfv98TjbuS"
   },
   "source": [
    "We can pass to our tokenizer the question and the context together, and it will properly insert the special tokens to form a sentence like this:\n",
    "\n",
    "[CLS] question [SEP] context [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "hb4A-5Zd9lBU",
    "outputId": "5990aa4f-5bb3-4b71-e27b-516f1dec0c07"
   },
   "outputs": [],
   "source": [
    "context = raw_datasets[\"train\"][0][\"context\"]\n",
    "question = raw_datasets[\"train\"][0][\"question\"]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiNrSZdYj0va"
   },
   "source": [
    "\n",
    "\n",
    "*   **max_length** to set the maximum length (here 100)\n",
    "*   **truncation=\"only_second\"** to truncate the context (which is in the second\n",
    "position) when the question with its context is too long\n",
    "*   **stride** to set the number of overlapping tokens between two successive chunks (here 50)\n",
    "*   **return_overflowing_tokens=True** to let the tokenizer know we want the overflowing tokens\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3P9OGlY9lBV",
    "outputId": "c48ef25e-6e97-42a4-bcf5-27f617ec6a37"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EP2LkYYlD7k"
   },
   "source": [
    "As we can see, our example has been in split into four inputs, each of them containing the question and some part of the context. Note that the answer to the question (‚ÄúBernadette Soubirous‚Äù) only appears in the third and last inputs, so by dealing with long contexts in this way we will create some training examples where the answer is not included in the context. For those examples, the labels will be **start_position = end_position = 0 (so we predict the [CLS] token)**. We will also set those labels in the unfortunate case where the answer has been truncated so that we only have the start (or end) of it. For the examples where the answer is fully in the context, the labels will be the index of the token where the answer starts and the index of the token where the answer ends.\n",
    "\n",
    "The dataset provides us with the start character of the answer in the context, and by adding the length of the answer, we can find the end character in the context. To map those to token indices, we will need to use the offset mappings we studied. We can have our tokenizer return these by passing along **return_offsets_mapping=True:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gt3uLQLK9lBV",
    "outputId": "c4519a21-6b62-451c-d639-1bdaa680645f"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOejWTscmqmq"
   },
   "source": [
    "As we can see, we get back the usual input IDs, token type IDs, and attention mask, as well as the offset mapping we required and an extra key, overflow_to_sample_mapping. The corresponding value will be of use to us when we tokenize several texts at the same time (which we should do to benefit from the fact that our tokenizer is backed by Rust). Since one sample can give several features, it maps each feature to the example it originated from. Because here we only tokenized one example, we get a list of 0s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_UI_4Rc9lBW",
    "outputId": "fd62f724-c1d4-4235-8be9-30e1e86ebd5c"
   },
   "outputs": [],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyBQKGin9lBX",
    "outputId": "467a766e-ebae-47cf-fc39-32ca748f88a5"
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ONQl3FOneWL"
   },
   "source": [
    "As we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave four features and the last example (at index 5 in the training set) gave 7 features.\n",
    "\n",
    "This information will be useful to map each feature we get to its corresponding label. As mentioned earlier, those labels are:\n",
    "\n",
    "*   **(0, 0)** if the answer is not in the corresponding span of the context\n",
    "*   **(start_position, end_position**) if the answer is in the corresponding span of the context, with **start_position** being the index of the token (in the input IDs) at the start of the answer and **end_position** being the index of the token (in the input IDs) where the answer ends\n",
    "\n",
    "To determine which of these is the case and, if relevant, the positions of the tokens, we first find the indices that start and end the context in the input IDs. We could use the token type IDs to do this, but since those do not necessarily exist for all models (DistilBERT does not require them, for instance), we‚Äôll instead use the sequence_ids() method of the BatchEncoding our tokenizer returns.\n",
    "\n",
    "Once we have those token indices, we look at the corresponding offsets, which are tuples of two integers representing the span of characters inside the original context. We can thus detect if the chunk of the context in this feature starts after the answer or ends before the answer begins (in which case the label is (0, 0)). If that‚Äôs not the case, we loop to find the first and last token of the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8uuYcRb9lBY",
    "outputId": "4744a222-6489-4f2e-ced6-5f3a01d4fb3f"
   },
   "outputs": [],
   "source": [
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rg74JiyoGdC"
   },
   "source": [
    "Let‚Äôs take a look at a few results to verify that our approach is correct. For the first feature we find (83, 85) as labels, so let‚Äôs compare the theoretical answer with the decoded span of tokens from 83 to 85 (inclusive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9xdsoaG9lBZ",
    "outputId": "ea1feb1e-2ac3-4d75-bdeb-42ed1d5c6a41"
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSlw8JcooWoZ"
   },
   "source": [
    "So that‚Äôs a match! Now let‚Äôs check index 4, where we set the labels to (0, 0), which means the answer is not in the context chunk of that feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "782zcc0w9lBa",
    "outputId": "34fe97f2-c88d-4449-b555-2b88f09ef7ca"
   },
   "outputs": [],
   "source": [
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
    "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNrfTaJqob5f"
   },
   "source": [
    "Indeed, we don‚Äôt see the answer inside the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O60S-X1ypDD8"
   },
   "source": [
    "‚úèÔ∏è Your turn! When using the XLNet architecture, padding is applied on the left and the question and context are switched. Adapt all the code we just saw to the XLNet architecture (and add padding=True). Be aware that the [CLS] token may not be at the 0 position with padding applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK5EYh1LpD2R",
    "outputId": "beb6c04d-9148-4562-9529-66b5d8019dcd"
   },
   "outputs": [],
   "source": [
    "# Tokenize for XLNet ‚Äî context comes first, padding on the left\n",
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],  # context first\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],  # question second\n",
    "    max_length=100,\n",
    "    truncation=\"only_first\",   # truncate context if needed\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    "    return_token_type_ids=False,  # XLNet does not use token_type_ids\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tl5uUljGpJLy"
   },
   "outputs": [],
   "source": [
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = start_char + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context in the token sequence\n",
    "    idx = 0\n",
    "    while idx < len(sequence_ids) and sequence_ids[idx] != 0:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "\n",
    "    while idx < len(sequence_ids) and sequence_ids[idx] == 0:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # Check if answer is in the current context span\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Find token positions of the answer start and end\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qNKx8WqpMkP",
    "outputId": "ec8694c5-af81-4e58-8d6f-4dfb69c44a94"
   },
   "outputs": [],
   "source": [
    "# For a matching example\n",
    "idx = 0\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")\n",
    "\n",
    "# For a non-matching example\n",
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
    "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQSryiZirPFq"
   },
   "source": [
    "| Element          | BERT-style (original)         | XLNet (updated)                 |\n",
    "| ---------------- | ----------------------------- | ------------------------------- |\n",
    "| Padding          | Right                         | Left                            |\n",
    "| Input order      | `[question] [SEP] [context]`  | `[context] [SEP] [question]`    |\n",
    "| Truncation       | `\"only_second\"` (context)     | `\"only_first\"` (context)        |\n",
    "| `token_type_ids` | Often used                    | Often ignored                   |\n",
    "| `[CLS]` position | At start                      | Not guaranteed (due to padding) |\n",
    "| `sequence_ids()` | 0 for question, 1 for context | 0 for context, 1 for question   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N96Q2Xk6p8cS"
   },
   "source": [
    "Now that we have seen step by step how to preprocess our training data, we can group it in a function we will apply on the whole training dataset. We‚Äôll pad every feature to the maximum length we set, as most of the contexts will be long (and the corresponding samples will be split into several features), so there is no real benefit to applying dynamic padding here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viF7wlnR9lBb"
   },
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjX_HSH6qOEp"
   },
   "source": [
    "Note that we defined two constants to determine the maximum length used as well as the length of the sliding window, and that we added a tiny bit of cleanup before tokenizing: some of the questions in the SQuAD dataset have extra spaces at the beginning and the end that don‚Äôt add anything (and take up space when being tokenized if you use a model like RoBERTa), so we removed those extra spaces.\n",
    "\n",
    "To apply this function to the whole training set, we use the **Dataset.map()** method with the **batched=True** flag. It‚Äôs necessary here as we are changing the length of the dataset (since one example can give several training features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "78a1af64f1964bea8344be1fd78cabef",
      "a9d2cf1dc75a4c698e9d211c39ec5ba9",
      "615ec4328f0640eb81f9903a52965b66",
      "2339f1a50a9a4e50b67eefa118d10bd6",
      "4bd1e58bd0c74ee0bb25ee551f62baba",
      "b97cc5d55e5548b2bdabb0393374a352",
      "b4162bc62c064ad9b6e4383d164dcf73",
      "fad35b75c5fb40c2925a80ca10152836",
      "44423538533349c1991809d72c88225f",
      "a9e20ec5aa234dee9f42aa64a32e408f",
      "8305005203bd4ab5ba686fee1bf34cd3"
     ]
    },
    "id": "I3DQfi4J9lBc",
    "outputId": "33ba9dea-09ce-4dd2-8cc1-3ba6ae00ffca"
   },
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12vq_exXqiFs"
   },
   "source": [
    "## Processing the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-onX-1Vq9AS"
   },
   "source": [
    "Preprocessing the validation data will be slightly easier as we don‚Äôt need to generate labels (unless we want to compute a validation loss, but that number won‚Äôt really help us understand how good the model is). The real joy will be to interpret the predictions of the model into spans of the original context. For this, we will just need to store both the offset mappings and some way to match each created feature to the original example it comes from. Since there is an ID column in the original dataset, we‚Äôll use that ID.\n",
    "\n",
    "The only thing we‚Äôll add here is a tiny bit of cleanup of the offset mappings. They will contain offsets for the question and the context, but once we‚Äôre in the post-processing stage we won‚Äôt have any way to know which part of the input IDs corresponded to the context and which part was the question (the **sequence_ids()** method we used is available for the output of the tokenizer only). So, we‚Äôll set the offsets corresponding to the question to **None**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7Ztn7LL9lBc"
   },
   "outputs": [],
   "source": [
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    # The method .strip() in Python is used to remove leading and trailing whitespace (including spaces, tabs, and newline characters) from a string.\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "0823d5c2661349b5b6b16b3671fd96a7",
      "928bddd7c3ad404ba319f4e655bf8686",
      "9b34c3ace17341209b99089a30e53c7f",
      "3a5e6cd0514a48188703668e65d9660d",
      "bedd0595eed8415aa0d86d6ff7024f1a",
      "61898310212741c3b955b4d882517ae3",
      "d02fa9a5d8d743308d01af5e5352efe7",
      "58f341776257475eb93a70123aba7756",
      "c07074acbdb5437fa16619c68f17a077",
      "76b0e028953d4b6f99ed9802508e008a",
      "1bc2a6e332d34ffbaa7b3baf80166fc0"
     ]
    },
    "id": "ofd39PPa9lBe",
    "outputId": "7c416bd4-830c-4622-be3c-8e392d9d2e6c"
   },
   "outputs": [],
   "source": [
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvwVmelor0xC"
   },
   "source": [
    "# Fine-tuning the model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QifuGUvsFvi"
   },
   "source": [
    "the hardest thing will be to write the compute_metrics() function. Since we padded all the samples to the maximum length we set, there is no data collator to define, so this metric computation is really the only thing we have to worry about. The difficult part will be to post-process the model predictions into spans of text in the original examples; once we have done that, the metric from the ü§ó Datasets library will do most of the work for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfvUZty7sHlj"
   },
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDwfIYC9tOTa"
   },
   "source": [
    "The model will output logits for the start and end positions of the answer in the input IDs, as we saw during our exploration of the question-answering pipeline. The post-processing step will be similar to what we did there, so here‚Äôs a quick reminder of the actions we took:\n",
    "\n",
    "*   We masked the start and end logits corresponding to tokens outside of the context.\n",
    "*   We then converted the start and end logits into probabilities using a softmax.\n",
    "\n",
    "*   We attributed a score to each (start_token, end_token) pair by taking the product of the corresponding two probabilities.\n",
    "*   We looked for the pair with the maximum score that yielded a valid answer (e.g., a start_token lower than end_token).\n",
    "\n",
    "Here we will change this process slightly because we don‚Äôt need to compute actual scores (just the predicted answer). This means we can skip the softmax step. To go faster, we also won‚Äôt score all the possible (start_token, end_token) pairs, but only the ones corresponding to the highest n_best logits (with n_best=20). Since we will skip the softmax, those scores will be logit scores, and will be obtained by taking the sum of the start and end logits (instead of the product, because of the rule\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "a\n",
    "b\n",
    ")\n",
    "=\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "a\n",
    ")\n",
    "+\n",
    "log\n",
    "‚Å°\n",
    "(\n",
    "b\n",
    ")\n",
    "log(ab)=log(a)+log(b)).\n",
    "\n",
    "To demonstrate all of this, we will need some kind of predictions. Since we have not trained our model yet, we are going to use the default model for the QA pipeline to generate some predictions on a small part of the validation set. We can use the same processing function as before; because it relies on the global constant tokenizer, we just have to change that object to the tokenizer of the model we want to use temporarily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "f0026b93d98949b6b572cb0ccf52c65e",
      "5ea23afd915241d9af6bf9099f405ef0",
      "27dc78e70deb454eb6d961bd0a76ff7a",
      "0325f07f03ec4f69a5258f9ca1c4b249",
      "2ba8982a9f5745d9b1938f5275519a0d",
      "0ffe579da92a4c359b6ffcb2b38a446f",
      "54a28262868c465aa974dc2717bb8afb",
      "bdae73e4de254c6c87c3e92341998c23",
      "9b98809b85904fb3b0d34df67a9898c7",
      "0dae6aed5f5341ccb9cbdc0cc00307d0",
      "7bd7d9c9742246d19b35ac302d4552cd",
      "be04cca61a7f44178cfd7ef7b39f69cf",
      "a01fe3eafe604fddadc485a768b03d57",
      "ea69a77ce5234f3daaf443b176df5d74",
      "79dbc475f97243899507ad59a9634abe",
      "cf4d8a9b23574f9ebcc5e827220f8b46",
      "adc12dabae30466b9223cd51c67e11c4",
      "d8fe37e07f00455cace73cc456bfc17c",
      "9672c3637dd740b69505f75378906419",
      "05e99dadb1424e03bd58ed0c1277acf6",
      "721e7bace3a240b18376dc6073faec7a",
      "7b3a5885c3e84265ba056a219f78a6bc",
      "9af84efc1ecb4748acfa812c1c4b019d",
      "1840aa6e966043a091a6d999effca18d",
      "c2c18ea6091448ddb9b3b43f6d10ec73",
      "45e048a6d8864defb99809ab407b6659",
      "008ccd96970f446a8a5ff4d6a10390ec",
      "349a7984acc04e0da5a138f768614dfc",
      "f8167578f7eb49c8a0535925ac1b4e2a",
      "d0338d4780784716a7cca118c63bdb92",
      "d8c8173e3eee4a5fb4a9e4dc6911509d",
      "07516ed40e914165bdfb94ac15757fa1",
      "ca2924b598454295af410a5c15242a16",
      "455517434927483493406c57585f5f3c",
      "f04fba9ed68c44788c36762c91d81733",
      "a5aaf31173394b19ba1aa9be53d91d81",
      "3e671c7ca0694882a03f001c9d8012e8",
      "78c316d393144de284afc08ba4c0a52b",
      "28236667b4ad4df0bb380c023786cb7f",
      "7d01de50b4854dff8a2d6bdb61833035",
      "dac17f4177f34de0b11f01149b15a6e3",
      "93d1f42dda5e4da0bec447f0541e6779",
      "efdba331df564a1e9733d83da0c73c93",
      "bdfab39ab02247efa7a32be1c7864edf",
      "3e09ae3027074ed39ccc7a08ce54e68f",
      "72c079504d294ccc92943f300a1d1793",
      "872aad1260054ed495d7a3447c468ec2",
      "a245bc81932040079d347ba384f77c2e",
      "0ae5f19ee1b244839ab75f40fa560599",
      "c6cee44b06b848abb4aa87fc4ca0e5ac",
      "c77e0d5b327d46c2b7031e092aa8f5a5",
      "49933d0bbee9406fadf89c1d30cf6cdc",
      "69ffc567ba5249e7aaa1d720648c4d6f",
      "7c4d69dc7fd84f8a87095a1222dcf29e",
      "00fad63d69ac4f519fd2ae93c26adbb1"
     ]
    },
    "id": "qwQGQkfS9lBe",
    "outputId": "04daa23a-7763-4a12-cc7a-4de3427ab451"
   },
   "outputs": [],
   "source": [
    "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWx_jUrvxDxu"
   },
   "source": [
    "Now that the preprocessing is done, we change the tokenizer back to the one we originally picked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XhXf9yH9lBf"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilpr5SdhxJiB"
   },
   "source": [
    "We then remove the columns of our eval_set that are not expected by the model, build a batch with all of that small validation set, and pass it through the model. If a GPU is available, we use it to go fas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_djJ2ur9lBg"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoModelForQuestionAnswering, default_data_collator\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "# # eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# # batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "# trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "#     device\n",
    "# )\n",
    "# eval_dataloader = DataLoader(\n",
    "#     eval_set_for_model, collate_fn=default_data_collator, batch_size=64\n",
    "# )\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in eval_dataloader:\n",
    "#         outputs = trained_model(**batch)\n",
    "\n",
    "# # with torch.no_grad():\n",
    "# #     outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "e05dbac057114603b24f5fe1e9583cfd",
      "4cb7471993a1407caa3dc5d374367e4d",
      "16484daed2c44b909ac2eb29edd9ec1b",
      "4903a4e87f32405dabf90f812ce88b4f",
      "3b424bae8c7c48a19dc13707771879cb",
      "f0191568a4714b4586bb381ab2842e74",
      "3f323519df7f4c2490a32a4c04a2a594",
      "e3591a3948dd473aa0538d7478e4bca8",
      "05ae7d93dadb49b6b3590aade631768a",
      "b1887f9aa15a4cbab4c1ee10ad6e45a4",
      "838f0c4e63e44a1aaeec340cfd817030"
     ]
    },
    "id": "OOtAHUu5Lj2x",
    "outputId": "a6c1eb90-4df7-45eb-84e7-0770266f49c0"
   },
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Remove unnecessary columns for model input\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_set_for_model,\n",
    "    batch_size=16,  # You can adjust this depending on your memory\n",
    "    collate_fn=default_data_collator\n",
    ")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the model\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint)\n",
    "trained_model.to(device)\n",
    "trained_model.eval()\n",
    "\n",
    "all_start_logits = []\n",
    "all_end_logits = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in eval_dataloader:\n",
    "        # Move each tensor in the batch to the device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Run the model\n",
    "        outputs = trained_model(**batch)\n",
    "\n",
    "        # Collect logits\n",
    "        all_start_logits.append(outputs.start_logits.cpu())\n",
    "        all_end_logits.append(outputs.end_logits.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rweM-yOFLr6I"
   },
   "outputs": [],
   "source": [
    "start_logits = torch.cat(all_start_logits, dim=0).numpy()\n",
    "end_logits = torch.cat(all_end_logits, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91pXRx3y9lBg"
   },
   "outputs": [],
   "source": [
    "# start_logits = outputs.start_logits.cpu().numpy()\n",
    "# end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4Uj9anS9lBg"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature[\"example_id\"]].append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cdk4KpR6M4U6"
   },
   "source": [
    "With this in hand, we can really get to work by looping through all the examples and, for each example, through all the associated features. As we said before, we‚Äôll look at the logit scores for the n_best start logits and end logits, excluding positions that give:\n",
    "\n",
    "*   An answer that wouldn‚Äôt be inside the context\n",
    "*   An answer with negative length\n",
    "*   An answer that is too long (we limit the possibilities at max_answer_length=30)\n",
    "\n",
    "\n",
    "Once we have all the scored possible answers for one example, we just pick the one with the best logit score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_2tyQ8d9lBi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "b69be184c589482f8266a85a05ef0fdd",
      "e4182e7db342442684dece36e325ba44",
      "33d9dc210476459a978b7de856a61643",
      "7606b5a4f58944aea9c41f85cdf7e243",
      "2de9b75463e24e33a682127ab443dae1",
      "8890c1439cfc4b23957f5a9f74a9bbcb",
      "3e44ecf0c5784acd8d1e0d85dba56962",
      "12a32ac9e38a411f968176c65eaa5c18",
      "7e8f103ccc8e4cf1a39e267ff0dc68c0",
      "805882c1ddd3465094c99c009900a06b",
      "4a2d4ab2d7834d4db491eda5c8bd810f",
      "c92531ba493a464d879414ff8bf68a24",
      "4bdff4d0e30f4d879bfbf1b13aced0fc",
      "dc9f2376f6334ffa82e0297ce70b8fd8",
      "222474a69f2a4048b8e479f8cf0a49f4",
      "f6fb21bf6b404a0eb86798d8f121d087",
      "8aee158856374957a0ba75c62409b40d",
      "9968dde835d745ee90250ed1f1ae1af4",
      "36c9fd0c1a844a05aaf6cb5b298f528e",
      "4784e7fdb51d427da837b65f162e6928",
      "5cdc08f1a9844adf8f5e1eb105f344e8",
      "d4f21266938041528f8f5296f171121a"
     ]
    },
    "id": "SCKZXKu09lBj",
    "outputId": "6b8fe837-3671-406a-ddd5-bd711c1e9492"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO48xRaa9lBk"
   },
   "outputs": [],
   "source": [
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtxe-rbS9lBk",
    "outputId": "375fe0c4-0c77-41b4-d3d4-d98888a8f4de"
   },
   "outputs": [],
   "source": [
    "print(predicted_answers[0])\n",
    "print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaULSFgE9lBm",
    "outputId": "01bfa6f5-eb22-4e97-d3b4-29a6102b84a5"
   },
   "outputs": [],
   "source": [
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRCBaoBRNn7p"
   },
   "source": [
    "Now let‚Äôs put everything we just did in a **compute_metrics()** function that we will use in the **Trainer**. Normally, that **compute_metrics(**) function only receives a tuple **eval_preds** with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won‚Äôt be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.\n",
    "\n",
    "The **compute_metrics(**) function groups the same steps as before; we just add a small check in case we don‚Äôt come up with any valid answers (in which case we predict an empty string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g47ndpm49lBn"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "8084c3754e9b43aaab8e73e6d6a0f6bb",
      "35d1f715324047a7be4e849dc10c50b5",
      "6ee810b772fd421aa40048ef97ebc72e",
      "a15b27096ffe428a9b712a7fb8e42308",
      "5d3c358cee28461fa46ed7383d27fb5d",
      "d67d50e6ea364d6ba61b4417a5475224",
      "e2bd8630b8d24991a739073c35e8e4e0",
      "f61054ef14b94b52a8ba104cd0994b72",
      "aba5bdb7b3354a7e84fc262964a6adf9",
      "ef50596e082c43e4a2fb15d8ba926f56",
      "376b5a15e845497bbf236ca594dc93c1"
     ]
    },
    "id": "gRCRajoU9lBo",
    "outputId": "e2c2fb00-a597-4492-c0a2-1564a71220a7"
   },
   "outputs": [],
   "source": [
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8Fo7HDoN8US"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "530d2303fa174c9cb7a00e5cdfb63c45",
      "369b51f890484f7ebb210ad48a249605",
      "0ab682ebd6444405bdb0354dd5f6d16c",
      "cc603e694dba450491e5b15787d090de",
      "5df5b8d11a8e4a7c824c8b872ce65d52",
      "fbbb0cf1e2eb4882bb42911f299c9200",
      "8ae6e51a5894429bac1a01c871fbb13b",
      "16cc163a41484ab28d516411f82dcb0f",
      "418b776fdd64401c8de63319f45119d7",
      "a6f24dc3fd0b44bf8305867587885fc1",
      "b2a30bc829e742de973b5b6843dafa7d"
     ]
    },
    "id": "uCdwDvn59lB3",
    "outputId": "f3721de6-26f2-4b97-f083-f0bf51243fc6"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj5U_3RW9lB4"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nYIFrDSzPq9S",
    "outputId": "4fd8376f-d416-4bed-fb02-59c4ebbad042"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwN2YCy3PS2g"
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVbY0qkt9lB5"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_Q7abYUv9lB6",
    "outputId": "6ca046aa-e542-493d-f539-e3f43ca4d792"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "8a3b8c4c0ec64ecdba108ba568570e48",
      "5c02ee46cc0b4acd9ccdbd21f34c2caa",
      "a92c9fdb2be643dcac16051ddcf42dd0",
      "1f9027a26b824ea79bbe1dd83b9a5d7e",
      "898ab9d69c3b439d9d107801484900b8",
      "300e7521ec524eb5ba08004173c17768",
      "9d900410ffb0419195a7ee3433a0000c",
      "8e41a4172cec4437bd9dac61731d77c2",
      "b8b3b63ed131457b8928bd3afd94b4bb",
      "f323238cda4048509046d74ae70eab88",
      "02769032ced44397aa3dd82dd0c7908f"
     ]
    },
    "id": "qRe4eMso9lB6",
    "outputId": "b00f8727-b637-4b0c-ea0f-cd1415c13830"
   },
   "outputs": [],
   "source": [
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "RBf25S8f9lB7",
    "outputId": "c099f6c0-5247-4f61-d274-ece979fc3e16"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcP2OJvVSk_-"
   },
   "source": [
    "# A custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9ssoKSrSuqi"
   },
   "source": [
    "## Preparing everything for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecTJstdwS2uI"
   },
   "source": [
    "First we need to build the **DataLoaders** from our datasets. We set the format of those datasets to **\"torch\"**, and remove the columns in the validation set that are not used by the model. Then, we can use the **default_data_collator** provided by Transformers as a **collate_fn** and shuffle the training set, but not the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3JavHwl9lB8"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Ah7g2uD9lB9",
    "outputId": "9a2aa575-9fbc-4b40-e9f9-04b857ea967a"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY-YwZQ7z4py"
   },
   "source": [
    "Then we will need an optimizer. As usual we use the classic AdamW, which is like Adam, but with a fix in the way weight decay is applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ci_26uuz9lB9"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04O6PBkO9lB-"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWA6nV0k0JwQ"
   },
   "source": [
    "As you should know from the previous sections, we can only use the **train_dataloader** length to compute the number of training steps after it has gone through the **accelerator.prepare()** method. We use the same linear schedule as in the previous sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uwBGGV_9lB_"
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "DW0YZ_NV9lB_",
    "outputId": "fd5db1a3-344f-4e80-c2b3-f9f352442b5d"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"bert-finetuned-squad-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "fb18745ed2c04107b893f55211074fa2",
      "dcc4c220417647228958538d986933d6",
      "1fe85896cfab42ae97c2692d37219123",
      "7825a96879b446049d994956f8207629",
      "72253406c7ee448190549011690a90a6",
      "220d3b18bd56488e854a7cc30d566b28",
      "bd8932daa41b4d9390b1cc34e93baf54",
      "8fd973761c5549bd9e818640845720b2",
      "823530433f574c6e86a7c285290d5d22",
      "0be2af441ec94a609a18f5ee3870347d",
      "56de69df852443c2b1ff6ef1833dafc8",
      "d6496d4db33d475381dd7ceb381186f9",
      "87c9f892879647d9b2d2798c4ec1a096",
      "d82e8afc2de84c5281d2be9fe96a122b",
      "aecbff3e23894318a0e67f78a0c9cf98",
      "1cb621ee027749aa960fa780aeb10bde",
      "620fda383fab42d681fbfcec869b7f52",
      "a15eec75101d4432b4215c2a9952cbd6",
      "cc732efbd02649719368f1e6874945d1",
      "1bfbccd4e1b94fbf9ef99cf0b0122748",
      "448bbc2697474433aff5b8bd88bf5055",
      "01de96f843ff43d79b3b6fd01d84125a"
     ]
    },
    "id": "sXGTgVV59lCA",
    "outputId": "dda4b166-08f7-4631-f680-e445b3fab9e3"
   },
   "outputs": [],
   "source": [
    "output_dir = \"bert-finetuned-squad-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOephifw0U-H"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH73yfGf0dQg"
   },
   "source": [
    "We are now ready to write the full training loop. After defining a progress bar to follow how training goes, the loop has three parts:\n",
    "\n",
    "*   The training in itself, which is the classic iteration over the train_dataloader, forward pass through the model, then backward pass and optimizer step.\n",
    "*   The evaluation, in which we gather all the values for start_logits and end_logits before converting them to NumPy arrays. Once the evaluation loop is finished, we concatenate all the results. Note that we need to truncate because the Accelerator may have added a few samples at the end to ensure we have the same number of examples in each process.\n",
    "*   Saving and uploading, where we first save the model and the tokenizer, then call repo.push_to_hub(). As we did before, we use the argument blocking=False to tell the ü§ó Hub library to push in an asynchronous process. This way, training continues normally and this (long) instruction is executed in the background.\n",
    "\n",
    "Here‚Äôs the complete code for the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "3c68f5caed94498395f1b87de9fdab51",
      "af3a085f62394eb9b6e0e44018c93f78",
      "fc402e5d554d484094e78809ccd3fcb7",
      "cf4581ef95834defb2d1e0644eb7b07e",
      "4849931798fb462595bb4eb30612a16d",
      "293b106405cc4cf88d157f525d956cdf",
      "e67741b5510c4555b5aba8f93dd2adfe",
      "ea08c4e6e76d461aa4a9a3d40d1625db",
      "f41b004d2df84357bdf296a86a661092",
      "3659f053bb3e471983346be7d7a7612d",
      "d7b719d125ee463680eaee58971532c7",
      "217d3612d5b04ee5a77485139d980c99",
      "af88dd77e7464fd8bd07d9f1668e24f7",
      "3132c874b2a548219c33deea6a09a48a",
      "8901c59c0c0e44429215e0bb646cfe15",
      "17f3659d532640348d49e4351f113b7e",
      "77a26acabc7d4d52a38334d4b4be6de7",
      "7471fd637b624abd89d508c5e4e330f7",
      "3457671e54144b21a90f096f51d08df8",
      "d5c3f392a70f49f7a24fe2fcbeb6b7c6",
      "82ee216bc6744cc88cce7a8ac82b2ba5",
      "b501cd04b44747d69d64b54234b7ab75",
      "2682a86f588b4a96bd577cea9fc68817",
      "94041e01fb6f4a5cbfa4080b80847efe",
      "699c84b13941410da01486e0c185f4ce",
      "0377566879124bd486243b963cea25b5",
      "4a376d59493b467097fea289577d3215",
      "a39563d49ed8485e96bd232a8efc93c8",
      "7368d846fa40437d924e8f69e9fb1640",
      "c85d2d4eb1514b028fc45cc22f554985",
      "24e620b939f04fb590d4040a7b035881",
      "9ad03e216d994de2bc179d422d0f3e7f",
      "2297c33fa55a4a8ea7babe9949269394",
      "dc909ceab85f449a82b916b2d211ab9d",
      "9f326609db2d4863bf904d0d997bda4e",
      "fec87dbb27c54888b9e5a70819d876cc",
      "5b889bc8ed894902b4eb6d896435946e",
      "8d71c739e172465fae331ddb95dcc58d",
      "997ce75a77e340ddadfb19222dcfc077",
      "dae09b8e8c344fbc9538390c092ba298",
      "365319b6da7b422eadbb5977b9626e0a",
      "cd2b166823364077810dc9d6aebbd85a",
      "7b5b3e8e42a74df094aad60e85d4819c",
      "29ce9f85043542bca21b66fa059aef63",
      "fc5d57a364df4b65bd87b2e49ddc90d0",
      "7637ccb2b0364c60a91b6c15cdfc579e",
      "91db790464204332bd62c1c475530f48",
      "573bd6f64c9645df8669b756b58657f8",
      "f30ed74909f34beea4a25057ffa84bf2",
      "717b53995a3e423a94e88f007e1201cf",
      "88367e37564a4ba099cbcf44acd1f1c1",
      "30687734486b4d5e9e8f6256cb69bf1e",
      "73eadffa947640bb89dd829a9a93fc24",
      "8893f09998084383a7494c827db25943",
      "10873cb53d124a048a968f7f9f61fea2",
      "6eb84ebbd2804f359badd5e73f40e5ab",
      "f98db1964b3e4a8494890a44d36ecbf0",
      "56a683badfaf4b52b6a45b7894b99042",
      "1314246394d54ed981bff3369e0465ae",
      "c0d6aed3427c400ab5d492ef9649f431",
      "0fb775b9df5a4c9f98e92fe3d0029d87",
      "ab8ce57f67e34d868c112a7e4409ce66",
      "a86dbed398ed47cfb56afcd3cd72681c",
      "9f61fd71cf0049f6b224c45fbd6b23b3",
      "e683cb37adf34aaf9be7c041db720a05",
      "de8f27e085a2448cac910648e38c052d",
      "08cd161ce5b84cdf9060d4b6626f45df",
      "289bf705e4f9437fad8ba5723105c6fa",
      "b8ebdc7edfcd4577aa2fe62d4f260551",
      "ad60f3bbc4ff482980900beba80e69c5",
      "dc23433cc0804e36bbf09502c8e56d59",
      "777f7178ca8c4cbb8e9ef9eb9ce36167",
      "0ba729214d504e80b6cbb04abd25fadc",
      "0f882e451f364e90851d94169063dbf8",
      "8900675c42284f1da6a335f9b32390e6",
      "77d2c1d5a8fa435abf18da24d88ce88a",
      "aaf43b0fe3c14c58b81261d8fa5101aa"
     ]
    },
    "id": "9Pyu7QMf9lCB",
    "outputId": "e774447a-7ffa-4ba7-a096-663e7fab439e"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n",
    "    )\n",
    "    print(f\"epoch {epoch}:\", metrics)\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkCPFCka9lCD"
   },
   "outputs": [],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeRHyzgw086X"
   },
   "source": [
    "The first line is self-explanatory: it tells all the processes to wait until everyone is at that stage before continuing. This is to make sure we have the same model in every process before saving. Then we grab the unwrapped_model, which is the base model we defined. The **accelerator.prepare()** method changes the model to work in distributed training, so it won‚Äôt have the **save_pretrained()** method anymore; the **accelerator.unwrap_model()** method undoes that step. Lastly, we call **save_pretrained()** but tell that method to use **accelerator.save()** instead of **torch.save()**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqMt37pW1KhQ"
   },
   "source": [
    "# Using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "14884ae5b85c402aa032706401e20724",
      "3393beba2d924c248470712445047f3a",
      "3a2e9c775db3471991a5f8cf1fab65ad",
      "b87a2abd41c943819dd9a7b3d68f79ed",
      "866bf6a20b0042aca050b7b4547b0130",
      "ab3e161f304f48a6a551a8c1a4426449",
      "4db80b511bfa4746ab6b85e5900447de",
      "9615e247a6bb4bf7a0b1bdaecc090f41",
      "af50f99dbc55492a9a42174143af314e",
      "e1e916db7194432e92a42a8f8975b92a",
      "3f54b40ab5364328b48d68e72e546a1a",
      "154dc12725fe45d2ada93aa829c2a759",
      "57385cc866f04e9abd0fd95ade9016ec",
      "aa8c19e54411436ebae64208d7a2e5ed",
      "8a7c59ac9142491a80c968a4a6218604",
      "e19ab85bbaf9424e930f0d2e085d7106",
      "31d130a610c249e99ee80bf75d1e5e1b",
      "ceb120c74a8845c69dcd7e1960553c79",
      "740bd39a2679460e86d7faa1f28eb86f",
      "0a9bf845ddac433683d7fee0749c8976",
      "3e3c1d4721b8402482cce308cb0b5894",
      "6fde552910824a3992d1ebd822976eb1",
      "6135df04592b4cd8bb5c4eefc7b9487e",
      "5601525920e54a579a23a7bc5e076140",
      "c1f3d37f0b274c36afcc86f966f85a6f",
      "58cb50dfc8dc48d081e886e39a869b8d",
      "95aa5cf9862642f3b38450576d3849e7",
      "cc1fbc8dadd04e89acb48677337f42a3",
      "d7019eddddfb45f7a85391a5a45cc592",
      "8084925c0eaa4f839efa5f3bf5e50f03",
      "0eef099b8f4a42e29113c3db95b30cde",
      "1d251bd1e2154e5dbd5945c5d0db6fe0",
      "eb9eab039d5941dab4f8997f3d3e92f7",
      "13eb284e4c1c4cd39e27c92435807619",
      "5b1f0d23f52845eea34567b418adec86",
      "8f853630ba1046eb98471d8ea76fe58d",
      "1e8d7f8c339c47c5944409dea2285a92",
      "1f509e439e344857bc2170f741d63cb7",
      "8a0536c6fe5142b2b1a558dd2963c20b",
      "6c1416510e964325adff6d8f5cf674f4",
      "eb1eb5f037ba4378bfc98b2c0b277d83",
      "8a184847b8124b9f9bef44c9f58228db",
      "50c4dcc8ca5d4bb598be01aa91cd4d82",
      "aab16e6a20034c8bb5bbb161f9364e5f",
      "53000e6c84c04647b7b70adf61574ac9",
      "129c04a209bf437797c4c5f69fa44bcd",
      "31f107d9066142b79ca81d431e373b1f",
      "5cd3883e66d3418990530375e8411d82",
      "61ebdd7025c64d7ba85973e9132a16ef",
      "cbddc5f236764768ab4da1115ce9c862",
      "a40beaaf82aa46f095d6ab35f5fe70f3",
      "2edaea558e214c6c87b22895598e8d45",
      "370db39dae784e5eb282b1528421d6db",
      "b72fec9135904e14b791cb4d57d32021",
      "183f1f02ee5b444daf9eade2840e9330",
      "f97bb6ceaf6441a2ab6c2d7dbfdb7125",
      "a22e96af46fe4cc2a23e688dfff45f16",
      "552f928c1b2547ccaf65c20661aa7800",
      "2c3cdf7aef8a4452be0f530ff39cf7b0",
      "976b43a6e1db41d481b6a12e3237336c",
      "559477548e4b4bd49818389e7cb2fd00",
      "0a52c6cbeb9f4bf6b55676d28684510a",
      "adba5220934c4a78a51cf86d61ab47e5",
      "e97f52e7d1784cfdbcb9f4a02618eb70",
      "ecdf0eff3f4f42b995fb9c359a27e516",
      "3ec17e6a3987452289c57723a6e4f4e2"
     ]
    },
    "id": "nxF5swCT9lCE",
    "outputId": "7c25adf8-aebb-46dd-bcb5-ea483e891a13"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"Mhammad2023/bert-finetuned-squad\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "9c2ff01345b74d79aac8dfab9c86b9fa",
      "9777a266c8c142bf8ffcfddfaef09bf0",
      "b0a19a8568f74f7a8d4d1e3fd8938959",
      "e50d9ae4c01b4f22965c2ceb55df521c",
      "e851cf6785ac47e086069cad545d2992",
      "5727124ea869408abcdb3be46ca63853",
      "43fe14273dbe4b8197e3b8200ee5eee9",
      "f6d14e806a6d49f482aa3c3eb56ed827",
      "c0bb9588c00b4d969030ab4e90618dee",
      "bef6d59ac09b4fb583ef4e9d141063a9",
      "f3c8e5016347428c8503c3cad21c2d06",
      "237726b7228e4d529b53d5c306a495b7",
      "faaae4741fad44bbb4ae145e51412658",
      "e257f24493df493aa32bb05b224b302f",
      "ecb51d3018c84252904bedeccc375f10",
      "40d1d7e6a7f3490cae0721d01a1a94d6",
      "a8f5b306260f489ea68ac208097e25d0",
      "eacf86eb240644a581b031f14982eedf",
      "b63749220f9e4a838a7dba324a2d3472",
      "085bf9d43fc645b098c9edb7e8fd1f0c",
      "1fe848603dda4a1faa414266f26a830d",
      "8b0ade08aa374d6b87d984b630c37c9b",
      "287f008a97404aa2ae07a78831ac4db6",
      "302cf59d3345495fab6c952ec14a8893",
      "dfcdd0fbb9f744e88b9991fe17ef8d76",
      "f993a7205f2d486eac4f5bbe0c7d5732",
      "37b0d139770a4749a8615fe207fbd686",
      "c1c49ea3f9c144119f0d76b121f5920f",
      "a03883c4d4f44c16843cd648a9194a6f",
      "2486833067c44fa081f5df43357c83bc",
      "04d2ab0710bb4acfbc8f5119ba5dec88",
      "5c90a943edb5418fbe9d8ced1cbde631",
      "8ea0b4351f4b43d09759164c6ee2c9f8",
      "19bbc7cfe0c8471682066e99787d2c7a",
      "17ada9b3f0af41319f5f0d095e2de43e",
      "7032fd22881048c98603ba5d741dbd00",
      "3c7a854e8fc74c078287517c0ab5a7f0",
      "20160de2661f45098f0d13f151416600",
      "273df939443f4570a50ddae9e297bb77",
      "6e93bdf89ffa4cf1aefd1cb995502dda",
      "376d05e21dcb416dad0de1c3117773ee",
      "282f2cb2af424fadb276f5b0015f0121",
      "309ddb07ff234e9996bc67f23d0e946b",
      "3f2fe82b3b0c4e518175fb9cb1edee80",
      "2d99b5332c884e65b53d1a6157519eb1",
      "19ca5212fa844e3596299c0f837a5675",
      "c9a82ee1d5f246099bccb9aa427ba88c",
      "4844303b37a1493bb7958f6aaae2844e",
      "f0956d22bd2048f389c39f009b16b8cb",
      "92e3dd58477a460f9ba0a89b54fef75c",
      "c12a03606a0e4b66b8b62f63b5a5a320",
      "d79a59ae771d4483a57613aa3f449843",
      "e61e18519f5549708fd9c0815544fcf3",
      "c8bccce807014f2fab24a8a040880ee5",
      "d8e8bf5117fd42478167d157df8be1ec",
      "d8840ca2a82743b487d3cda491790083",
      "a85e363213974ace82a11c4a96717ed0",
      "e901904bde2d49849f0160494ebc5ef7",
      "cd44258d347742169085e033320051f2",
      "2446b37b35ab4024b3be26c4888d2f9c",
      "f4a0c9fba5de452d9a895c5b34b88116",
      "a42077f51377412ba0f577c3b1105b4c",
      "9a2e0100713041d5ad002bd33c4dcf1c",
      "608805da8bce4ccab5cc9344e601089c",
      "5d81fb4326424f8198aadda524d732d6",
      "933672eb0bb3431eaaf834271df880de"
     ]
    },
    "id": "nVPvqg5qvvpj",
    "outputId": "2bf38919-c45a-40f7-ec25-521eaf027019"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"Mhammad2023/bert-finetuned-squad-accelerate\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint)\n",
    "\n",
    "context = \"\"\"\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3PUqOjDvzLS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
